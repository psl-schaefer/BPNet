{
  "hash": "9d4eb891a39b5a377732128187764a7d",
  "result": {
    "markdown": "---\ntitle: 04 Architecture Comparisons\nexecute:\n  freeze: auto\nformat:\n  html:\n    html-math-method: mathjax\n    theme: darkly\n    toc: true\n    number-sections: true\n    code-tools:\n      source: repo\ndescription: 'Comparing different number of dilated convolutional layers, different number of channels in each dilated convolutational layer, and different kernel sizes for the first layer.'\n---\n\n# Train Parameters\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nTF_LIST = [\"Nanog\", \"Klf4\", \"Oct4\", \"Sox2\"]\nINPUT_DIR = \"/home/philipp/BPNet/input/\"\nBATCH_SIZE = 64\nALPHA = 10\nMAX_EPOCHS = 100  \nEARLY_STOP_PATIENCE = 4\nRESTORE_BEST_WEIGHTS = True\nplot_while_train = False\nretrain_conv_layers = False\nretrain_channel = False\nretrain_kern_size = False\n```\n:::\n\n\n# Libraries\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.style.use('dark_background')\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom src.architectures import BPNet\nfrom src.utils import ChIP_Nexus_Dataset, dummy_shape_predictions, dummy_total_counts_predictions\nfrom src.loss import neg_log_multinomial\nfrom src.metrics import permute_array, bin_max_values, bin_counts_amb, binary_labels_from_counts, compute_auprc_bins\ncolor_pal = {\"Oct4\": \"#CD5C5C\", \"Sox2\": \"#849EEB\", \"Nanog\": \"#FFE03F\", \"Klf4\": \"#92C592\", \"patchcap\": \"#827F81\"}\nimport sklearn.metrics as skm\nimport pandas as pd\nimport seaborn as sns\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing cuda device\n```\n:::\n:::\n\n\n# Data\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntrain_dataset = ChIP_Nexus_Dataset(set_name=\"train\", \n                                   input_dir=INPUT_DIR, \n                                   TF_list=TF_LIST)\ntrain_dataset\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 93904\n```\n:::\n:::\n\n\nDetermine $\\lambda$ hyperparameter\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nlambda_param = (np.median(train_dataset.tf_counts.sum(axis=-1), axis=0)).mean() / 10\nlambda_param\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n11.7375\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndummy_shape_predictions(train_dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnfiform Prediction Loss:\t490.46\nMean Prediction Loss:\t\t438.73\nPerfect Prediction Loss:\t133.78\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndummy_total_counts_predictions(train_dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Prediction Loss:\t\t0.71\nPerfect Prediction Loss:\t0.00\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntune_dataset = ChIP_Nexus_Dataset(set_name=\"tune\", \n                                  input_dir=INPUT_DIR, \n                                  TF_list=TF_LIST)\ntune_dataset\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nChIP_Nexus_Dataset\nSet: tune\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 29277\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndummy_shape_predictions(tune_dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnfiform Prediction Loss:\t494.43\nMean Prediction Loss:\t\t442.46\nPerfect Prediction Loss:\t135.56\n```\n:::\n:::\n\n\n# Different Number of Dilated Convolutational Layers\n\n## Training (Only Shape Prediction)\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nn_layers_list = np.arange(1,16)\n\nif retrain_conv_layers:\n  for n_layers in n_layers_list:\n    model = BPNet(n_dil_layers=n_layers, TF_list=TF_LIST, pred_total=False, bias_track=True).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\n    train_loader=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n    tune_loader=DataLoader(tune_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\n    train_loss, test_loss = [], []\n    patience_counter = 0\n\n    for epoch in range(MAX_EPOCHS):\n      model.train()\n      train_loss_epoch = []\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        optimizer.zero_grad()\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n        train_loss_epoch.append(loss.item())\n        loss.backward()\n        optimizer.step()\n      train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n      # evaluation part\n      test_loss_epoch = []\n      with torch.no_grad():\n          for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n              one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n              profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n              loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n              test_loss_epoch.append(loss.item())\n          test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n      if test_loss[-1] > np.array(test_loss).min():\n        patience_counter += 1\n      else:\n        patience_counter = 0\n        best_state_dict = model.state_dict()\n\n      if patience_counter == EARLY_STOP_PATIENCE:\n        break\n    \n    if RESTORE_BEST_WEIGHTS:\n      model.load_state_dict(best_state_dict)\n\n    # plot train and test loss\n    plt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\n    plt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # save the model\n    torch.save(obj=model, f=f\"/home/philipp/BPNet/trained_models/{n_layers}_dil_layers_model.pt\")\n```\n:::\n\n\n## Evaluation\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=INPUT_DIR, \n                                  TF_list=TF_LIST)\ntest_dataset\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nsave_scores = []\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\ntrue_counts = test_dataset.tf_counts.copy()\nfor n in n_layers_list:\n  model = torch.load(f\"/home/philipp/BPNet/trained_models/{n}_dil_layers_model.pt\")\n  # make predictions\n  pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\n  with torch.no_grad():\n      for batch_idx, data in enumerate(test_dataloader):\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          pred[batch_idx, :, :, :] = profile_pred\n  all_pred = pred.cpu().numpy().copy()\n  assert np.allclose(all_pred.sum(axis=-1), 1)\n          \n  for i, tf in enumerate(TF_LIST): # loop over the four TFs\n      pred_tf = all_pred[:, i, :, :]\n      counts_tf = true_counts[:, i, :, :]\n      labels, predictions, random = binary_labels_from_counts(counts_tf, pred_tf)\n      auprc_score = skm.average_precision_score(labels, predictions)\n      save_scores.append({\"tf\": tf,\n                          \"n_layers\":n,\n                          \"auprc\": auprc_score})\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndf = pd.DataFrame(save_scores)\ndf.to_csv(\"/home/philipp/BPNet/out/dil_layers_auprc.csv\")\nsns.scatterplot(data=df, x=\"n_layers\", y=\"auprc\", hue=\"tf\", palette=color_pal)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](04_architecture_comparison_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\n# Different Number of Dilated Convolutational Layers\n\n## Training (Only Shape Prediction)\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nn_channel_list = np.array([2, 4, 8, 16, 32, 64, 128, 256])\n\nif retrain_channel:\n  for n_channel in n_channel_list:\n    model = BPNet(n_dil_layers=9, TF_list=TF_LIST, pred_total=False, bias_track=True, conv_channels=n_channel).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\n    train_loader=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n    tune_loader=DataLoader(tune_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\n    train_loss, test_loss = [], []\n    patience_counter = 0\n\n    for epoch in range(MAX_EPOCHS):\n      model.train()\n      train_loss_epoch = []\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        optimizer.zero_grad()\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n        train_loss_epoch.append(loss.item())\n        loss.backward()\n        optimizer.step()\n      train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n      # evaluation part\n      test_loss_epoch = []\n      with torch.no_grad():\n          for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n              one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n              profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n              loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n              test_loss_epoch.append(loss.item())\n          test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n      if test_loss[-1] > np.array(test_loss).min():\n        patience_counter += 1\n      else:\n        patience_counter = 0\n        best_state_dict = model.state_dict()\n\n      if patience_counter == EARLY_STOP_PATIENCE:\n        break\n    \n    if RESTORE_BEST_WEIGHTS:\n      model.load_state_dict(best_state_dict)\n\n    # plot train and test loss\n    plt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\n    plt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # save the model\n    torch.save(obj=model, f=f\"/home/philipp/BPNet/trained_models/{n_channel}_conv_channel_model.pt\")\n```\n:::\n\n\n## Evaluation\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=INPUT_DIR, \n                                  TF_list=TF_LIST)\ntest_dataset\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nsave_scores = []\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\ntrue_counts = test_dataset.tf_counts.copy()\nfor n in n_channel_list:\n  model = torch.load(f\"/home/philipp/BPNet/trained_models/{n}_conv_channel_model.pt\")\n  # make predictions\n  pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\n  with torch.no_grad():\n      for batch_idx, data in enumerate(test_dataloader):\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          pred[batch_idx, :, :, :] = profile_pred\n  all_pred = pred.cpu().numpy().copy()\n  assert np.allclose(all_pred.sum(axis=-1), 1)\n          \n  for i, tf in enumerate(TF_LIST): # loop over the four TFs\n      pred_tf = all_pred[:, i, :, :]\n      counts_tf = true_counts[:, i, :, :]\n      labels, predictions, random = binary_labels_from_counts(counts_tf, pred_tf)\n      auprc_score = skm.average_precision_score(labels, predictions)\n      save_scores.append({\"tf\": tf,\n                          \"n_channels\":n,\n                          \"auprc\": auprc_score})\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndf = pd.DataFrame(save_scores)\ndf.to_csv(\"/home/philipp/BPNet/out/conv_channel_auprc.csv\")\nsns.scatterplot(data=df, x=\"n_channels\", y=\"auprc\", hue=\"tf\", palette=color_pal)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](04_architecture_comparison_files/figure-html/cell-18-output-1.png){}\n:::\n:::\n\n\n# Different Sizes of the first Kernel\n\n## Training (Only Shape Prediction)\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nkernel_sizes = np.array([5, 9, 13, 17, 21, 25, 29, 33, 37])\n\nif retrain_kern_size:\n  for kern_size in kernel_sizes:\n    model = BPNet(n_dil_layers=9, TF_list=TF_LIST, pred_total=False, bias_track=True, conv_channels=64, size_first_kernel=kern_size).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\n    train_loader=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n    tune_loader=DataLoader(tune_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\n    train_loss, test_loss = [], []\n    patience_counter = 0\n\n    for epoch in range(MAX_EPOCHS):\n      model.train()\n      train_loss_epoch = []\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        optimizer.zero_grad()\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n        train_loss_epoch.append(loss.item())\n        loss.backward()\n        optimizer.step()\n      train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n      # evaluation part\n      test_loss_epoch = []\n      with torch.no_grad():\n          for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n              one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n              profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n              loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n              test_loss_epoch.append(loss.item())\n          test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n      if test_loss[-1] > np.array(test_loss).min():\n        patience_counter += 1\n      else:\n        patience_counter = 0\n        best_state_dict = model.state_dict()\n\n      if patience_counter == EARLY_STOP_PATIENCE:\n        break\n    \n    if RESTORE_BEST_WEIGHTS:\n      model.load_state_dict(best_state_dict)\n\n    # plot train and test loss\n    plt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\n    plt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # save the model\n    torch.save(obj=model, f=f\"/home/philipp/BPNet/trained_models/{kern_size}_first_kern_size_model.pt\")\n```\n:::\n\n\n## Evaluation\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=INPUT_DIR, \n                                  TF_list=TF_LIST)\ntest_dataset\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n```\n:::\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nsave_scores = []\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\ntrue_counts = test_dataset.tf_counts.copy()\nfor n in kernel_sizes:\n  model = torch.load(f\"/home/philipp/BPNet/trained_models/{n}_first_kern_size_model.pt\")\n  # make predictions\n  pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\n  with torch.no_grad():\n      for batch_idx, data in enumerate(test_dataloader):\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          pred[batch_idx, :, :, :] = profile_pred\n  all_pred = pred.cpu().numpy().copy()\n  assert np.allclose(all_pred.sum(axis=-1), 1)\n          \n  for i, tf in enumerate(TF_LIST): # loop over the four TFs\n      pred_tf = all_pred[:, i, :, :]\n      counts_tf = true_counts[:, i, :, :]\n      labels, predictions, random = binary_labels_from_counts(counts_tf, pred_tf)\n      auprc_score = skm.average_precision_score(labels, predictions)\n      save_scores.append({\"tf\": tf,\n                          \"first_kern_size\": n,\n                          \"auprc\": auprc_score})\n```\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ndf = pd.DataFrame(save_scores)\ndf.to_csv(\"/home/philipp/BPNet/out/first_kern_size_auprc.csv\")\nsns.scatterplot(data=df, x=\"first_kern_size\", y=\"auprc\", hue=\"tf\", palette=color_pal)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](04_architecture_comparison_files/figure-html/cell-22-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "04_architecture_comparison_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}