[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "What data do we have, what are we predicting and why are we doing that?\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nFrom raw ChIP-seq data to model input.\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nDocumentation for our dataset and neural network architecture API.\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nTraining BPNet to predict the ChIP-Nexus profiles and total counts for all four TFs.\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nComparing different number of dilated convolutional layers, different number of channels in each dilated convolutational layer, and different kernel sizes for the first layer.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "00_introduction.html",
    "href": "00_introduction.html",
    "title": "00 Introduction",
    "section": "",
    "text": "Notes:\n\nActually they used an adaption of ChIP-seq called ChIP-seq Nexus which comprises an additional exonuclease step yielding an increased resolution."
  },
  {
    "objectID": "01_prepare_input.html",
    "href": "01_prepare_input.html",
    "title": "01 Prepare Input",
    "section": "",
    "text": "Sys.setenv(RETICULATE_PYTHON = \"/home/philipp/miniconda3/envs/r-reticulate/bin/python3\")\nsuppressPackageStartupMessages({\n  library(reticulate)\n  library(rtracklayer)\n  library(GenomicAlignments)\n  library(BRGenomics)\n  library(BSgenome.Mmusculus.UCSC.mm10)\n  library(tidyverse)\n  library(furrr)\n  library(motifmatchr)\n  library(JASPAR2020)\n  library(TFBSTools)\n})\n# Setup Multiprocessing\noptions(future.globals.maxSize=5000*1024^2)\nfuture::plan(future::multisession, workers = 4)"
  },
  {
    "objectID": "01_prepare_input.html#output-directory",
    "href": "01_prepare_input.html#output-directory",
    "title": "01 Prepare Input",
    "section": "2.1 Output Directory",
    "text": "2.1 Output Directory\n\noutput_dir <- \"/home/philipp/BPNet/input/\"\nif (!dir.exists(output_dir)){dir.create(output_dir)}\nTFs <- c(\"Sox2\", \"Oct4\", \"Klf4\", \"Nanog\")\npurrr::walk(c(TFs, \"patchcap\", \"figures\"), ~ if (!dir.exists(paste0(output_dir, .x))){dir.create(paste0(output_dir, .x))})"
  },
  {
    "objectID": "01_prepare_input.html#peak-width",
    "href": "01_prepare_input.html#peak-width",
    "title": "01 Prepare Input",
    "section": "2.2 Peak Width",
    "text": "2.2 Peak Width\nAs in the BPNet paper, we are using a peak width of 1000 bp, meaning we consider 500 bp up- and downstream of the ChIP-seq peaks.\n\npeak_width <- 1000"
  },
  {
    "objectID": "01_prepare_input.html#chromosome-traintunetest-split",
    "href": "01_prepare_input.html#chromosome-traintunetest-split",
    "title": "01 Prepare Input",
    "section": "2.3 Chromosome Train/Tune/Test split",
    "text": "2.3 Chromosome Train/Tune/Test split\n\nall_chroms <- paste0(\"chr\", c(1:19, \"X\", \"Y\"))\n\n# we use the same train/tune/test split as in the BPnet paper\nchrom_list <- list(\"tune\" = c(\"chr2\", \"chr3\", \"chr4\"), # tune set (hyperparameter tuning): chromosomes 2, 3, 4\n                   \"test\" = c(\"chr1\", \"chr8\", \"chr9\")) # test set (performance evaluation): chromosome 1, 8, 9\nchrom_list$train <- setdiff(all_chroms, c(chrom_list$tune, chrom_list$test)) # train set: all other chromosomes\nchrom_list\n\n$tune\n[1] \"chr2\" \"chr3\" \"chr4\"\n\n$test\n[1] \"chr1\" \"chr8\" \"chr9\"\n\n$train\n [1] \"chr5\"  \"chr6\"  \"chr7\"  \"chr10\" \"chr11\" \"chr12\" \"chr13\" \"chr14\" \"chr15\"\n[10] \"chr16\" \"chr17\" \"chr18\" \"chr19\" \"chrX\"  \"chrY\""
  },
  {
    "objectID": "01_prepare_input.html#colors",
    "href": "01_prepare_input.html#colors",
    "title": "01 Prepare Input",
    "section": "2.4 Colors",
    "text": "2.4 Colors\n\ncolors = c(\"Klf4\" = \"#92C592\",\n           \"Nanog\" = \"#FFE03F\",\n           \"Oct4\" = \"#CD5C5C\",\n           \"Sox2\" = \"#849EEB\",\n           \"patchcap\" = \"#827F81\")"
  },
  {
    "objectID": "01_prepare_input.html#distribution-of-the-macs2-scores",
    "href": "01_prepare_input.html#distribution-of-the-macs2-scores",
    "title": "01 Prepare Input",
    "section": "5.1 Distribution of the MACS2 Scores",
    "text": "5.1 Distribution of the MACS2 Scores\n\np <- peak_infos %>%\n  as.data.frame() %>%\n  ggplot() +\n  geom_density(aes(x=log2(qValue), color=TF), alpha=0.4, fill=NA) +\n  labs(x=\"Log2 qValue\", y=\"Density\", color=\"TF\") +\n  scale_color_manual(values=colors) +\n  theme_bw()\nggsave(filename = \"/home/philipp/BPNet/out/figures/macs2_scores_per_tf.pdf\", \n       plot = p, width = 4, height = 4)\np +\n  ggdark::dark_theme_linedraw()"
  },
  {
    "objectID": "01_prepare_input.html#number-of-peaks-per-tf",
    "href": "01_prepare_input.html#number-of-peaks-per-tf",
    "title": "01 Prepare Input",
    "section": "5.2 Number of Peaks per TF",
    "text": "5.2 Number of Peaks per TF\n\np <- peak_infos %>%\n  as.data.frame() %>%\n  ggplot() +\n  geom_bar(aes(y=TF, fill=TF), alpha=0.4, width=0.4, color=\"black\") +\n  labs(x=\"Number of Peaks\", y=\"TF\") +\n  theme_bw() +\n  scale_fill_manual(values=colors)\nggsave(filename = \"/home/philipp/BPNet/out/figures/n_peaks_per_tf.pdf\", \n       plot = p, width = 4, height = 4)\np +\n  ggdark::dark_theme_linedraw()"
  },
  {
    "objectID": "01_prepare_input.html#examples-highest-scoring-peaks",
    "href": "01_prepare_input.html#examples-highest-scoring-peaks",
    "title": "01 Prepare Input",
    "section": "7.1 Examples: Highest Scoring Peaks",
    "text": "7.1 Examples: Highest Scoring Peaks\n\n\nClick to expand\n\n\ntest_df <- peak_infos %>%\n  as.data.frame() %>%\n  dplyr::filter(set==\"train\") %>%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %>%\n  dplyr::group_by(TF) %>%\n  slice_max(order_by=qValue, n=5) %>%\n  select(Region, TF, qValue)\n\npurrr::walk(1:nrow(test_df), function(i) {\n  test_instance <- unlist(test_df[i, ])\n  purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                 minus = -tf_counts[[tf]]$train$neg[match(test_instance[\"Region\"], seq_names$train), ]) %>%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %>% \n  pivot_longer(cols=c(minus, plus)) %>%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=name)) +\n  facet_wrap(~TF, scales=\"free_y\") +\n  ggdark::dark_theme_linedraw() +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=paste0(\"Peak \", test_instance[\"Region\"], \" | \", test_instance[\"TF\"], \n                    \" | \", test_instance[\"qValue\"])) -> p\n  print(p)\n})"
  },
  {
    "objectID": "01_prepare_input.html#examples-random-peaks",
    "href": "01_prepare_input.html#examples-random-peaks",
    "title": "01 Prepare Input",
    "section": "7.2 Examples: Random Peaks",
    "text": "7.2 Examples: Random Peaks\n\n\nClick to expand\n\n\nset.seed(42)\ntest_df <- peak_infos %>%\n  as.data.frame() %>%\n  dplyr::filter(set==\"train\") %>%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %>%\n  dplyr::group_by(TF) %>%\n  slice_sample(n=5) %>%\n  select(Region, TF, qValue)\n\npurrr::walk(1:nrow(test_df), function(i) {\n  test_instance <- unlist(test_df[i, ])\n  purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                 minus = -tf_counts[[tf]]$train$neg[match(test_instance[\"Region\"], seq_names$train), ]) %>%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %>% \n  pivot_longer(cols=c(minus, plus)) %>%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=name)) +\n  facet_wrap(~TF, scales=\"free_y\") +\n  ggdark::dark_theme_linedraw() +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=paste0(\"Peak \", test_instance[\"Region\"], \" | \", test_instance[\"TF\"], \n                    \" | \", test_instance[\"qValue\"])) -> p\n  print(p)\n})"
  },
  {
    "objectID": "01_prepare_input.html#examples-highest-scoring-peaks-with-bias",
    "href": "01_prepare_input.html#examples-highest-scoring-peaks-with-bias",
    "title": "01 Prepare Input",
    "section": "8.1 Examples: Highest Scoring Peaks with Bias",
    "text": "8.1 Examples: Highest Scoring Peaks with Bias\n\ntest_instance <- peak_infos %>%\n  as.data.frame() %>%\n  dplyr::filter(set==\"test\") %>%\n  dplyr::filter(seqnames==\"chr1\", start >= 180924752-1000, end <= 180925152+1000) %>%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %>%\n  dplyr::select(Region, TF, qValue) %>%\n  .[1, ]\nprint(test_instance)\n\n                    Region   TF   qValue\n1 chr1:180924435-180925434 Sox2 436.0653\n\np <- purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$test$pos[match(test_instance[\"Region\"], seq_names$test), ],\n                 minus = -tf_counts[[tf]]$test$neg[match(test_instance[\"Region\"], seq_names$test), ]) %>%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %>% \n  rbind(\n    tibble::tibble(position=-499:500, \n                   plus = ctrl_counts$test$pos[match(test_instance[\"Region\"], seq_names$test), ],\n                   minus = -ctrl_counts$test$neg[match(test_instance[\"Region\"], seq_names$test), ],\n                   TF = \"patchcap\", p_name = test_instance[\"Region\"])\n  ) %>%\n  pivot_longer(cols=c(minus, plus)) %>%\n  dplyr::mutate(TF = factor(TF, levels=names(colors))) %>%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=TF, alpha=name), size=0.2) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=test_instance[\"Region\"]) +\n  #scale_color_manual(values=c(\"minus\"=\"darkred\", \"plus\"=\"forestgreen\")) +\n  scale_color_manual(values=colors) +\n  scale_alpha_manual(values=c(\"plus\" = 1, \"minus\" = 1)) +\n  theme_bw() +\n  theme(plot.title = element_text(size=10, hjust=0.5), strip.background = element_rect(fill=NA))\nggsave(filename = \"/home/philipp/BPNet/out/figures/example_high_q.pdf\", \n       plot = p, width = 4, height = 8)\np + \n  ggdark::dark_theme_linedraw()\n\n\n\n\n\n\nClick to view more plots\n\n\ntest_df <- peak_infos %>%\n  as.data.frame() %>%\n  dplyr::filter(set==\"train\") %>%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %>%\n  dplyr::group_by(TF) %>%\n  slice_max(order_by=qValue, n=5) %>%\n  select(Region, TF, qValue)\npurrr::walk(1:nrow(test_df), function(i) {\n  test_instance <- unlist(test_df[i, ])\n  purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                 minus = -tf_counts[[tf]]$train$neg[match(test_instance[\"Region\"], seq_names$train), ]) %>%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %>% \n  rbind(\n    tibble::tibble(position=-499:500, \n                   plus = ctrl_counts$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                   minus = -ctrl_counts$train$neg[match(test_instance[\"Region\"], seq_names$train), ],\n                   TF = \"Bias\", p_name = test_instance[\"Region\"])\n  ) %>%\n  pivot_longer(cols=c(minus, plus)) %>%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=name)) +\n  facet_wrap(~TF, scales=\"free_y\") +\n  ggdark::dark_theme_linedraw() +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=paste0(\"Peak \", test_instance[\"Region\"], \" | \", test_instance[\"TF\"], \n                    \" | \", test_instance[\"qValue\"])) -> p\n  print(p)\n})"
  },
  {
    "objectID": "01_prepare_input.html#examples-random-peaks-with-bias",
    "href": "01_prepare_input.html#examples-random-peaks-with-bias",
    "title": "01 Prepare Input",
    "section": "8.2 Examples: Random Peaks with Bias",
    "text": "8.2 Examples: Random Peaks with Bias\n\ntest_instance <- peak_infos %>%\n  as.data.frame() %>%\n  dplyr::filter(set==\"test\") %>%\n  dplyr::filter(seqnames==\"chr1\", start >= 4000, end <= 100000000) %>%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %>%\n  dplyr::select(Region, TF, qValue) %>%\n  .[1000, ]\nprint(test_instance)\n\n                     Region   TF   qValue\n1000 chr1:58392603-58393602 Oct4 21.10429\n\np <- purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$test$pos[match(test_instance[\"Region\"], seq_names$test), ],\n                 minus = -tf_counts[[tf]]$test$neg[match(test_instance[\"Region\"], seq_names$test), ]) %>%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %>% \n  rbind(\n    tibble::tibble(position=-499:500, \n                   plus = ctrl_counts$test$pos[match(test_instance[\"Region\"], seq_names$test), ],\n                   minus = -ctrl_counts$test$neg[match(test_instance[\"Region\"], seq_names$test), ],\n                   TF = \"patchcap\", p_name = test_instance[\"Region\"])\n  ) %>%\n  pivot_longer(cols=c(minus, plus)) %>%\n  dplyr::mutate(TF = factor(TF, levels=names(colors))) %>%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=TF, alpha=name), size=0.2) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=test_instance[\"Region\"]) +\n  #scale_color_manual(values=c(\"minus\"=\"darkred\", \"plus\"=\"forestgreen\")) +\n  scale_color_manual(values=colors) +\n  scale_alpha_manual(values=c(\"plus\" = 1, \"minus\" = 1)) +\n  theme_bw() +\n  theme(plot.title = element_text(size=10, hjust=0.5), strip.background = element_rect(fill=NA))\nggsave(filename = \"/home/philipp/BPNet/out/figures/example_low_q.pdf\", \n       plot = p, width = 4, height = 8)\np + \n  ggdark::dark_theme_linedraw()\n\n\n\n\n\n\nClick to view more plots\n\n\nset.seed(42)\ntest_df <- peak_infos %>%\n  as.data.frame() %>%\n  dplyr::filter(set==\"train\") %>%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %>%\n  dplyr::group_by(TF) %>%\n  slice_sample(n=5) %>%\n  select(Region, TF, qValue)\n\npurrr::walk(1:nrow(test_df), function(i) {\n  test_instance <- unlist(test_df[i, ])\n  purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                 minus = -tf_counts[[tf]]$train$neg[match(test_instance[\"Region\"], seq_names$train), ]) %>%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %>% \n  rbind(\n    tibble::tibble(position=-499:500, \n                   plus = ctrl_counts$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                   minus = -ctrl_counts$train$neg[match(test_instance[\"Region\"], seq_names$train), ],\n                   TF = \"Bias\", p_name = test_instance[\"Region\"])\n  ) %>%\n  pivot_longer(cols=c(minus, plus)) %>%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=name)) +\n  facet_wrap(~TF, scales=\"free_y\") +\n  ggdark::dark_theme_linedraw() +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=paste0(\"Peak \", test_instance[\"Region\"], \" | \", test_instance[\"TF\"], \n                    \" | \", test_instance[\"qValue\"])) -> p\n  print(p)\n})"
  },
  {
    "objectID": "01_prepare_input.html#from-raw-data",
    "href": "01_prepare_input.html#from-raw-data",
    "title": "01 Prepare Input",
    "section": "9.1 From Raw Data",
    "text": "9.1 From Raw Data\n\nroi <- list(\"seqname\"=\"chr1\", \"start\"=180924752, \"end\"=180925152)\nTFs <- c(\"Oct4\", \"Sox2\", \"Nanog\", \"Klf4\")\ndf <-\n  purrr::map_dfr(TFs, function(tf) {\n    alignments <- readGAlignments(paste0(\"../data/chip-nexus/\", tf, \"/pool_filt.bam\"),\n                                  param = ScanBamParam(which=GRanges(paste0(roi$seqname, \":\", roi$start, \"-\", roi$end))))\n    align_pos <- alignments[strand(alignments)==\"+\"]\n    align_neg <- alignments[strand(alignments)==\"-\"]\n\n    align_pos@cigar <- rep(\"1M\", length(align_pos))\n    align_neg@start <- GenomicAlignments::end(align_neg)\n    align_neg@cigar <- rep(\"1M\", length(align_neg))\n\n    tibble::tibble(pos=as.numeric(coverage(align_pos)$chr1[roi$start:roi$end]),\n                   neg=-as.numeric(coverage(align_neg)$chr1[roi$start:roi$end]),\n                   position=roi$start:roi$end,\n                   TF=tf)\n  }) %>%\n  pivot_longer(cols=c(\"pos\", \"neg\"))\ndf\n\n# A tibble: 3,208 x 4\n    position TF    name  value\n       <int> <chr> <chr> <dbl>\n 1 180924752 Oct4  pos       0\n 2 180924752 Oct4  neg       0\n 3 180924753 Oct4  pos       2\n 4 180924753 Oct4  neg       0\n 5 180924754 Oct4  pos       2\n 6 180924754 Oct4  neg       0\n 7 180924755 Oct4  pos       0\n 8 180924755 Oct4  neg       0\n 9 180924756 Oct4  pos       0\n10 180924756 Oct4  neg       0\n# ... with 3,198 more rows\n# i Use `print(n = ...)` to see more rows\n\n\n\ndf %>%\n  mutate(TF = factor(TF, levels=c(TFs))) %>%\n  ggplot() +\n  geom_line(aes(x=position, y=value, col=name)) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  ggdark::dark_theme_linedraw()"
  },
  {
    "objectID": "01_prepare_input.html#from-processed-data",
    "href": "01_prepare_input.html#from-processed-data",
    "title": "01 Prepare Input",
    "section": "9.2 From Processed Data",
    "text": "9.2 From Processed Data\n\nroi <- list(\"seqname\"=\"chr1\", \"start\"=180924752, \"end\"=180925152)\nroi_adjusted <- roi\nroi_adjusted$start = roi_adjusted$start-1000\nroi_adjusted$end = roi_adjusted$end+1000\n\nTFs <- c(\"Oct4\", \"Sox2\", \"Nanog\", \"Klf4\")\ndf2 <- \n  purrr::map_dfr(TFs, function(tf) {\n    cov_list <- list(\"pos\" = tf_counts[[tf]]$test$pos,\n                     \"neg\" = tf_counts[[tf]]$test$neg)\n    rnames <- seq_names$test\n    gr_rnames <- GRanges(rnames)\n    \n    bool_vec <- \n      (as.character(gr_rnames@seqnames) == roi_adjusted$seqname &\n      start(gr_rnames@ranges) >= roi_adjusted$start &\n      end(gr_rnames@ranges) <= roi_adjusted$end)\n    \n    peak_index <- which(bool_vec)[1]\n    peak_info <- gr_rnames[peak_index]\n    \n    diff <- 180924752 - start(peak_info@ranges) + 1\n    w <- 400\n    \n    tibble::tibble(pos=cov_list$pos[peak_index, diff:(diff+w)],\n                   neg=-cov_list$neg[peak_index, diff:(diff+w)],\n                   position=0:400,\n                   TF=tf)\n}) %>%\n  pivot_longer(cols=c(\"pos\", \"neg\"))\ndf2\n\n# A tibble: 3,208 x 4\n   position TF    name  value\n      <int> <chr> <chr> <dbl>\n 1        0 Oct4  pos       0\n 2        0 Oct4  neg       0\n 3        1 Oct4  pos       2\n 4        1 Oct4  neg       0\n 5        2 Oct4  pos       2\n 6        2 Oct4  neg       0\n 7        3 Oct4  pos       0\n 8        3 Oct4  neg       0\n 9        4 Oct4  pos       0\n10        4 Oct4  neg       0\n# ... with 3,198 more rows\n# i Use `print(n = ...)` to see more rows\n\n\n\ndf2 %>%\n  mutate(TF = factor(TF, levels=c(TFs))) %>%\n  ggplot() +\n  geom_line(aes(x=position, y=value, col=name)) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  ggdark::dark_theme_linedraw()\n\n\n\n\nCompare.\n\nall(df2$value == df$value)\n\n[1] TRUE\n\nall(df2$TF == df$TF)\n\n[1] TRUE\n\nall(df2$name == df$name)\n\n[1] TRUE"
  },
  {
    "objectID": "01_prepare_input.html#save-image",
    "href": "01_prepare_input.html#save-image",
    "title": "01 Prepare Input",
    "section": "11.1 Save Image",
    "text": "11.1 Save Image\n\nsave.image(paste0(output_dir, \"tmp.RData\"))"
  },
  {
    "objectID": "01_prepare_input.html#session-info",
    "href": "01_prepare_input.html#session-info",
    "title": "01 Prepare Input",
    "section": "11.2 Session Info",
    "text": "11.2 Session Info\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.5 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0\n\nlocale:\n[1] C\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] TFBSTools_1.34.0                   JASPAR2020_0.99.10                \n [3] motifmatchr_1.18.0                 furrr_0.3.0                       \n [5] future_1.27.0                      forcats_0.5.1                     \n [7] stringr_1.4.0                      dplyr_1.0.9                       \n [9] purrr_0.3.4                        readr_2.1.2                       \n[11] tidyr_1.2.0                        tibble_3.1.8                      \n[13] ggplot2_3.3.6                      tidyverse_1.3.2                   \n[15] BSgenome.Mmusculus.UCSC.mm10_1.4.3 BSgenome_1.64.0                   \n[17] BRGenomics_1.8.0                   GenomicAlignments_1.32.1          \n[19] Rsamtools_2.12.0                   Biostrings_2.64.0                 \n[21] XVector_0.36.0                     SummarizedExperiment_1.26.1       \n[23] Biobase_2.56.0                     MatrixGenerics_1.8.1              \n[25] matrixStats_0.62.0                 rtracklayer_1.56.1                \n[27] GenomicRanges_1.48.0               GenomeInfoDb_1.32.3               \n[29] IRanges_2.30.0                     S4Vectors_0.34.0                  \n[31] BiocGenerics_0.42.0                reticulate_1.26                   \n\nloaded via a namespace (and not attached):\n  [1] readxl_1.4.0                backports_1.4.1            \n  [3] plyr_1.8.7                  splines_4.2.1              \n  [5] BiocParallel_1.30.3         listenv_0.8.0              \n  [7] digest_0.6.29               htmltools_0.5.3            \n  [9] GO.db_3.15.0                fansi_1.0.3                \n [11] magrittr_2.0.3              memoise_2.0.1              \n [13] googlesheets4_1.0.0         tzdb_0.3.0                 \n [15] globals_0.16.0              annotate_1.74.0            \n [17] modelr_0.1.8                R.utils_2.12.0             \n [19] colorspace_2.0-3            blob_1.2.3                 \n [21] rvest_1.0.2                 haven_2.5.0                \n [23] xfun_0.32                   crayon_1.5.1               \n [25] RCurl_1.98-1.8              jsonlite_1.8.0             \n [27] genefilter_1.78.0           TFMPvalue_0.0.8            \n [29] survival_3.4-0              glue_1.6.2                 \n [31] gtable_0.3.0                gargle_1.2.0               \n [33] zlibbioc_1.42.0             DelayedArray_0.22.0        \n [35] ggdark_0.2.1                plyranges_1.16.0           \n [37] scales_1.2.0                DBI_1.1.3                  \n [39] Rcpp_1.0.9                  xtable_1.8-4               \n [41] bit_4.0.4                   httr_1.4.3                 \n [43] RColorBrewer_1.1-3          ellipsis_0.3.2             \n [45] pkgconfig_2.0.3             XML_3.99-0.10              \n [47] R.methodsS3_1.8.2           farver_2.1.1               \n [49] dbplyr_2.2.1                locfit_1.5-9.6             \n [51] utf8_1.2.2                  tidyselect_1.1.2           \n [53] labeling_0.4.2              rlang_1.0.4                \n [55] reshape2_1.4.4              AnnotationDbi_1.58.0       \n [57] munsell_0.5.0               cellranger_1.1.0           \n [59] tools_4.2.1                 cachem_1.0.6               \n [61] cli_3.3.0                   DirichletMultinomial_1.38.0\n [63] generics_0.1.3              RSQLite_2.2.16             \n [65] broom_1.0.0                 evaluate_0.16              \n [67] fastmap_1.1.0               yaml_2.3.5                 \n [69] knitr_1.39                  bit64_4.0.5                \n [71] fs_1.5.2                    caTools_1.18.2             \n [73] KEGGREST_1.36.3             R.oo_1.25.0                \n [75] poweRlaw_0.70.6             pracma_2.3.8               \n [77] xml2_1.3.3                  compiler_4.2.1             \n [79] png_0.1-7                   reprex_2.0.1               \n [81] geneplotter_1.74.0          stringi_1.7.8              \n [83] lattice_0.20-45             CNEr_1.32.0                \n [85] Matrix_1.5-1                vctrs_0.4.1                \n [87] pillar_1.8.0                lifecycle_1.0.1            \n [89] bitops_1.0-7                R6_2.5.1                   \n [91] BiocIO_1.6.0                parallelly_1.32.1          \n [93] codetools_0.2-18            gtools_3.9.3               \n [95] assertthat_0.2.1            seqLogo_1.62.0             \n [97] DESeq2_1.36.0               rjson_0.2.21               \n [99] withr_2.5.0                 GenomeInfoDbData_1.2.8     \n[101] parallel_4.2.1              hms_1.1.1                  \n[103] grid_4.2.1                  rmarkdown_2.14             \n[105] googledrive_2.0.0           lubridate_1.8.0            \n[107] restfulr_0.0.15"
  },
  {
    "objectID": "02_APIs.html",
    "href": "02_APIs.html",
    "title": "02 Dataset and NN Architecture API",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('dark_background')\nimport seaborn as sns\nimport pandas as pd\nimport re\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom src.utils import ChIP_Nexus_Dataset\nfrom src.architectures import BPNet"
  },
  {
    "objectID": "02_APIs.html#example-1-create-train-dataset-for-all-tfs",
    "href": "02_APIs.html#example-1-create-train-dataset-for-all-tfs",
    "title": "02 Dataset and NN Architecture API",
    "section": "2.1 Example 1: Create Train Dataset for all TFs",
    "text": "2.1 Example 1: Create Train Dataset for all TFs\nOne has to provide the set which must be one of “train”, “tune”, “test” as well as the input directory and the list of TFs one wants to model.\n\nwhole_dataset = ChIP_Nexus_Dataset(set_name=\"train\", \n                                   input_dir=INPUT_DIR, \n                                   TF_list=['Sox2', 'Oct4', 'Klf4', 'Nanog'])\nwhole_dataset\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Sox2', 'Oct4', 'Klf4', 'Nanog']\nSize: 93904\n\n\nCheck the shapes via the check_shapes() method.\n\nwhole_dataset.check_shapes()\n\nself.tf_list=['Sox2', 'Oct4', 'Klf4', 'Nanog']\nself.one_hot_seqs.shape=(93904, 4, 1000) [idx, bases, pwidth]\nself.tf_counts.shape=(93904, 4, 2, 1000) [idx, TF, strand, pwidth]\nself.ctrl_counts.shape=(93904, 2, 1000) [idx, strand, pwidth]\nself.ctrl_counts_smooth.shape=(93904, 2, 1000) [idx, strand, pwidth]"
  },
  {
    "objectID": "02_APIs.html#example-2-create-train-dataset-for-sox2",
    "href": "02_APIs.html#example-2-create-train-dataset-for-sox2",
    "title": "02 Dataset and NN Architecture API",
    "section": "2.2 Example 2: Create Train Dataset for Sox2",
    "text": "2.2 Example 2: Create Train Dataset for Sox2\nIf we only want to take one or a few TFs into consideration we can specify which ones using the TF_list parameter. The constructor method will take care of everything and only keep the peaks that are specific to the TFs in the TF_list.\n\nsmall_dataset = ChIP_Nexus_Dataset(set_name=\"train\", \n                                   input_dir=INPUT_DIR, \n                                   TF_list=['Sox2'])\nsmall_dataset\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Sox2']\nSize: 6748\n\n\n\nsmall_dataset.check_shapes()\n\nself.tf_list=['Sox2']\nself.one_hot_seqs.shape=(6748, 4, 1000) [idx, bases, pwidth]\nself.tf_counts.shape=(6748, 1, 2, 1000) [idx, TF, strand, pwidth]\nself.ctrl_counts.shape=(6748, 2, 1000) [idx, strand, pwidth]\nself.ctrl_counts_smooth.shape=(6748, 2, 1000) [idx, strand, pwidth]"
  },
  {
    "objectID": "02_APIs.html#example-3-create-train-dataset-for-sox2-and-high-confidence-peaks",
    "href": "02_APIs.html#example-3-create-train-dataset-for-sox2-and-high-confidence-peaks",
    "title": "02 Dataset and NN Architecture API",
    "section": "2.3 Example 3: Create Train Dataset for Sox2 and High-Confidence Peaks",
    "text": "2.3 Example 3: Create Train Dataset for Sox2 and High-Confidence Peaks\nWe might also want to filter peaks based on the qValue.\n\ncutoff = 4.5\nsns.histplot(np.log2(small_dataset.region_info.qValue))\nplt.xlabel(\"Log2 qValue\")\nplt.title(\"Distribution of qValues\")\nplt.axvline(cutoff, color=\"red\")\nplt.show()\n\n\n\n\nLooking at the histogram of the log2 qValue, we might decide to only keep peaks with a log2 qValue above 4.5.\n\nhighconf_dataset = ChIP_Nexus_Dataset(set_name=\"train\", \n                                      input_dir=INPUT_DIR, \n                                      TF_list=[\"Sox2\"],\n                                      qval_thr=2**cutoff)\nhighconf_dataset\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Sox2']\nSize: 4182\n\n\n\nhighconf_dataset.check_shapes()\n\nself.tf_list=['Sox2']\nself.one_hot_seqs.shape=(4182, 4, 1000) [idx, bases, pwidth]\nself.tf_counts.shape=(4182, 1, 2, 1000) [idx, TF, strand, pwidth]\nself.ctrl_counts.shape=(4182, 2, 1000) [idx, strand, pwidth]\nself.ctrl_counts_smooth.shape=(4182, 2, 1000) [idx, strand, pwidth]"
  },
  {
    "objectID": "02_APIs.html#example-4-create-train-dataset-for-sox2-but-keep-all-regions",
    "href": "02_APIs.html#example-4-create-train-dataset-for-sox2-but-keep-all-regions",
    "title": "02 Dataset and NN Architecture API",
    "section": "2.4 Example 4: Create Train Dataset for Sox2 but keep all Regions",
    "text": "2.4 Example 4: Create Train Dataset for Sox2 but keep all Regions\nNow we might also want to create a training set that contains all the regions but only the counts for Sox2.\n\nsox2_all_regions = ChIP_Nexus_Dataset(set_name=\"train\", \n                                      input_dir=INPUT_DIR, \n                                      TF_list=[\"Sox2\"], \n                                      subset=False)\nsox2_all_regions\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Sox2']\nSize: 93904\n\n\n\nsox2_all_regions.check_shapes()\n\nself.tf_list=['Sox2']\nself.one_hot_seqs.shape=(93904, 4, 1000) [idx, bases, pwidth]\nself.tf_counts.shape=(93904, 1, 2, 1000) [idx, TF, strand, pwidth]\nself.ctrl_counts.shape=(93904, 2, 1000) [idx, strand, pwidth]\nself.ctrl_counts_smooth.shape=(93904, 2, 1000) [idx, strand, pwidth]"
  },
  {
    "objectID": "02_APIs.html#example-1-one-tf-shape-prediction-no-bias-track",
    "href": "02_APIs.html#example-1-one-tf-shape-prediction-no-bias-track",
    "title": "02 Dataset and NN Architecture API",
    "section": "3.1 Example 1: One TF, Shape Prediction, No Bias Track",
    "text": "3.1 Example 1: One TF, Shape Prediction, No Bias Track\n\nmodel_1 = BPNet(n_dil_layers=9, TF_list=[\"Sox2\"], pred_total=False, bias_track=False)\nmodel_1\n\nBPNet(\n  (base_model): ConvLayers(\n    (conv_layers): ModuleList(\n      (0): Conv1d(4, 64, kernel_size=(25,), stride=(1,), padding=same)\n      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n      (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(16,))\n      (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(32,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(64,))\n      (7): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(128,))\n      (8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(256,))\n      (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(512,))\n    )\n  )\n  (profile_heads): ModuleList(\n    (0): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n  )\n)"
  },
  {
    "objectID": "02_APIs.html#example-2-one-tf-shape-total-counts-prediction-no-bias-track",
    "href": "02_APIs.html#example-2-one-tf-shape-total-counts-prediction-no-bias-track",
    "title": "02 Dataset and NN Architecture API",
    "section": "3.2 Example 2: One TF, Shape & Total Counts Prediction, No Bias Track",
    "text": "3.2 Example 2: One TF, Shape & Total Counts Prediction, No Bias Track\n\nmodel_2 = BPNet(n_dil_layers=9, TF_list=[\"Sox2\"], pred_total=True, bias_track=False)\nmodel_2\n\nBPNet(\n  (base_model): ConvLayers(\n    (conv_layers): ModuleList(\n      (0): Conv1d(4, 64, kernel_size=(25,), stride=(1,), padding=same)\n      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n      (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(16,))\n      (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(32,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(64,))\n      (7): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(128,))\n      (8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(256,))\n      (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(512,))\n    )\n  )\n  (profile_heads): ModuleList(\n    (0): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n  )\n  (count_heads): ModuleList(\n    (0): TotalCountHead(\n      (fc1): Linear(in_features=64, out_features=32, bias=True)\n      (fc2): Linear(in_features=32, out_features=2, bias=True)\n    )\n  )\n)"
  },
  {
    "objectID": "02_APIs.html#example-3-one-tf-shape-total-counts-prediction-bias",
    "href": "02_APIs.html#example-3-one-tf-shape-total-counts-prediction-bias",
    "title": "02 Dataset and NN Architecture API",
    "section": "3.3 Example 3: One TF, Shape & Total Counts Prediction, Bias",
    "text": "3.3 Example 3: One TF, Shape & Total Counts Prediction, Bias\n\nmodel_3 = BPNet(n_dil_layers=9, TF_list=[\"Sox2\"], pred_total=True, bias_track=True)\nmodel_3\n\nBPNet(\n  (base_model): ConvLayers(\n    (conv_layers): ModuleList(\n      (0): Conv1d(4, 64, kernel_size=(25,), stride=(1,), padding=same)\n      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n      (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(16,))\n      (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(32,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(64,))\n      (7): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(128,))\n      (8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(256,))\n      (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(512,))\n    )\n  )\n  (profile_heads): ModuleList(\n    (0): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n  )\n  (count_heads): ModuleList(\n    (0): TotalCountHead(\n      (fc1): Linear(in_features=64, out_features=32, bias=True)\n      (fc2): Linear(in_features=32, out_features=2, bias=True)\n    )\n  )\n)\n\n\nFeatures bias weights.\n\nmodel_3.profile_heads[0].bias_weights\n\nParameter containing:\ntensor([0.0100, 0.0100], requires_grad=True)"
  },
  {
    "objectID": "02_APIs.html#example-4-all-tfs-shape-total-counts-prediction-bias",
    "href": "02_APIs.html#example-4-all-tfs-shape-total-counts-prediction-bias",
    "title": "02 Dataset and NN Architecture API",
    "section": "3.4 Example 4: All TFs, Shape & Total Counts Prediction, Bias",
    "text": "3.4 Example 4: All TFs, Shape & Total Counts Prediction, Bias\n\nmodel_4 = BPNet(n_dil_layers=9, TF_list=[\"Sox2\", \"Oct4\", \"Nanog\", \"Klf4\"], pred_total=True, bias_track=True)\nmodel_4\n\nBPNet(\n  (base_model): ConvLayers(\n    (conv_layers): ModuleList(\n      (0): Conv1d(4, 64, kernel_size=(25,), stride=(1,), padding=same)\n      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n      (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(16,))\n      (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(32,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(64,))\n      (7): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(128,))\n      (8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(256,))\n      (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(512,))\n    )\n  )\n  (profile_heads): ModuleList(\n    (0): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n    (1): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n    (2): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n    (3): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n  )\n  (count_heads): ModuleList(\n    (0): TotalCountHead(\n      (fc1): Linear(in_features=64, out_features=32, bias=True)\n      (fc2): Linear(in_features=32, out_features=2, bias=True)\n    )\n    (1): TotalCountHead(\n      (fc1): Linear(in_features=64, out_features=32, bias=True)\n      (fc2): Linear(in_features=32, out_features=2, bias=True)\n    )\n    (2): TotalCountHead(\n      (fc1): Linear(in_features=64, out_features=32, bias=True)\n      (fc2): Linear(in_features=32, out_features=2, bias=True)\n    )\n    (3): TotalCountHead(\n      (fc1): Linear(in_features=64, out_features=32, bias=True)\n      (fc2): Linear(in_features=32, out_features=2, bias=True)\n    )\n  )\n)"
  },
  {
    "objectID": "02_APIs.html#recreate-figure-1-e",
    "href": "02_APIs.html#recreate-figure-1-e",
    "title": "02 Dataset and NN Architecture API",
    "section": "4.1 Recreate Figure 1 e",
    "text": "4.1 Recreate Figure 1 e\n\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=INPUT_DIR, \n                                  TF_list=['Oct4', 'Sox2', 'Nanog', 'Klf4'])\ntest_dataset\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Oct4', 'Sox2', 'Nanog', 'Klf4']\nSize: 27727\n\n\n\ntmp_df = test_dataset.region_info.copy().reset_index()\nidx = tmp_df.loc[(tmp_df.seqnames==\"chr1\") & (tmp_df.start > 180924752-1000) & (tmp_df.end < 180925152+1000)].index.to_numpy()[0]\n\ndiff = 180924752 - tmp_df.start[idx] + 1\nw = 400\n\nfig, axis = plt.subplots(4, 1, figsize=(6, 14))\n\nfor ax, (i, tf) in zip(axis, enumerate(test_dataset.tf_list)):\n  ax.plot(test_dataset.tf_counts[idx, i, 0, diff:(diff+w)], label=\"pos\")\n  ax.plot(-test_dataset.tf_counts[idx, i, 1, diff:(diff+w)], label=\"neg\")\n  ax.legend()\n  ax.set_title(tf)\nplt.show()"
  },
  {
    "objectID": "02_APIs.html#check-one-hot-encoding",
    "href": "02_APIs.html#check-one-hot-encoding",
    "title": "02 Dataset and NN Architecture API",
    "section": "4.2 Check One-Hot Encoding",
    "text": "4.2 Check One-Hot Encoding\nTo check whether the one-hot encoding worked as expected, we compare here:\n\nThe one-hot encoded sequence as stored in the test dataset\n\n\nplt.imshow(test_dataset.one_hot_seqs[idx, :, diff:(diff+w)], interpolation=\"none\", aspect=\"auto\")\nplt.title(\"One-Hot Encoding from Test Dataset\")\nplt.yticks([0, 1, 2, 3], labels=[\"A\", \"C\", \"G\", \"T\"])\nplt.show()\n\n\n\n\n\nThe one-hot encoded sequence obtained from reading in the mm10 genome and one-hot encoding corresponding sequence\n\n\nfrom Bio.Seq import Seq\nfrom Bio import SeqIO\nmm10_ref = SeqIO.to_dict(SeqIO.parse(f\"../ref/mm10.fa\", \"fasta\"))\nseq = mm10_ref[tmp_df.iloc[idx][\"seqnames\"]][180924752:180925152]\none_hot_seq = np.zeros((4, 400))\nfor i, letter in enumerate(np.array(seq.seq)):\n  if letter==\"A\": one_hot_seq[0, i] = 1\n  if letter==\"C\": one_hot_seq[1, i] = 1\n  if letter==\"G\": one_hot_seq[2, i] = 1\n  if letter==\"T\": one_hot_seq[3, i] = 1\nplt.imshow(one_hot_seq, interpolation=\"none\", aspect=\"auto\")\nplt.yticks([0, 1, 2, 3], labels=[\"A\", \"C\", \"G\", \"T\"])\nplt.title(\"One-Hot Encoding based on Reference Sequence\")\nplt.show()\n\n\n\n\nfor the peak seen in Figure 1e\n\nnp.all(test_dataset.one_hot_seqs[idx, :, diff:(diff+w)] == one_hot_seq)\n\nTrue\n\n\nAnd we see that we get exactly the same."
  },
  {
    "objectID": "03_training.html",
    "href": "03_training.html",
    "title": "03 Training and Metrics",
    "section": "",
    "text": "TF_LIST = [\"Nanog\", \"Klf4\", \"Oct4\", \"Sox2\"]\n#TF_LIST = [\"Nanog\"]\nINPUT_DIR = \"/home/philipp/BPNet/input/\"\nBATCH_SIZE = 64\nMAX_EPOCHS = 100  \nEARLY_STOP_PATIENCE = 4\nRESTORE_BEST_WEIGHTS = True\nplot_while_train = False"
  },
  {
    "objectID": "03_training.html#train-loop",
    "href": "03_training.html#train-loop",
    "title": "03 Training and Metrics",
    "section": "4.1 Train Loop",
    "text": "4.1 Train Loop\n\nmodel = BPNet(n_dil_layers=9, TF_list=TF_LIST, pred_total=False, bias_track=True).to(device)\noptimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\ntrain_loader=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\ntune_loader=DataLoader(tune_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\ntrain_loss, test_loss = [], []\npatience_counter = 0\n\nfor epoch in range(MAX_EPOCHS):\n\n  # test\n  test_loss_epoch = []\n  with torch.no_grad():\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n          test_loss_epoch.append(loss.item())\n      test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n  # train\n  model.train()\n  train_loss_epoch = []\n  for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n    one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n    optimizer.zero_grad()\n    profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n    loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n    train_loss_epoch.append(loss.item())\n    loss.backward()\n    optimizer.step()\n  train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n  if test_loss[-1] > np.array(test_loss).min():\n    patience_counter += 1\n  else:\n    patience_counter = 0\n    best_state_dict = model.state_dict()\n\n  if patience_counter == EARLY_STOP_PATIENCE:\n    break\n\nif RESTORE_BEST_WEIGHTS:\n  model.load_state_dict(best_state_dict)"
  },
  {
    "objectID": "03_training.html#train-and-tune-loss",
    "href": "03_training.html#train-and-tune-loss",
    "title": "03 Training and Metrics",
    "section": "4.2 Train and Tune Loss",
    "text": "4.2 Train and Tune Loss\n\ndf = pd.DataFrame({\"epoch\": np.arange(1, epoch+2), \"train\": train_loss, \"test\": test_loss})\ndf.to_csv(\"/home/philipp/BPNet/out/shape_loss.csv\")\nplt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\nplt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "03_training.html#save-model",
    "href": "03_training.html#save-model",
    "title": "03 Training and Metrics",
    "section": "4.3 Save Model",
    "text": "4.3 Save Model\n\ntorch.save(obj=model, f=\"/home/philipp/BPNet/trained_models/all_tfs_shape_model.pt\")"
  },
  {
    "objectID": "03_training.html#evaluation",
    "href": "03_training.html#evaluation",
    "title": "03 Training and Metrics",
    "section": "4.4 Evaluation",
    "text": "4.4 Evaluation\n\nmodel = torch.load(\"/home/philipp/BPNet/trained_models/all_tfs_shape_model.pt\")\n\n\n4.4.1 Check Examples\nPlotting the real counts and the predictions for the first batch from the tune dataset.\n\ntune_loader=DataLoader(tune_dataset, batch_size=10, shuffle=False, num_workers=0, pin_memory=True)\none_hot, tf_counts, ctrl_counts, ctrl_smooth = next(tune_loader.__iter__())\none_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\nprofile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth).to(\"cpu\").detach().numpy()\ntf_counts = tf_counts.to(\"cpu\").detach().numpy()\nscaled_pred = profile_pred * tf_counts.sum(axis=-1)[:,:,:,None]\nlw = 0.8\n\n\n4.4.1.1 Nanog\n\ntf = 0\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.1.2 Klf4\n\ntf = 1\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.1.3 Oct4\n\ntf = 2\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.1.4 Sox2\n\ntf = 3\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 Precision and Recall\n\ntest_pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\nwith torch.no_grad():\n    for batch_idx, data in enumerate(test_dataloader):\n        #print(batch_idx, batch_idx + 100)#, data)\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        #print(profile_pred.shape)\n        start = batch_idx*BATCH_SIZE\n        end = (batch_idx+1)*BATCH_SIZE if (batch_idx+1)*BATCH_SIZE < test_dataset.tf_counts.shape[0] else test_dataset.tf_counts.shape[0]\n        test_pred[start:end, :, :, :] = profile_pred\n\n\ndef plot_prc(test_dataset, test_pred, tf_index, tf_name, plot = True):\n    true_counts = test_dataset.tf_counts.copy()\n    #subset for one tf\n    tf_counts = true_counts[:, tf_index, :, :]\n    test_pred = test_pred.cpu().numpy().copy()\n    assert np.allclose(test_pred.sum(axis=-1), 1)\n    # subset for one tf\n    tf_pred = test_pred[:, tf_index, :, :]\n    binary_labels, pred_subset, random = binary_labels_from_counts(tf_counts, tf_pred, verbose=False)\n    precision, recall, thresholds = skm.precision_recall_curve(binary_labels, pred_subset)\n    if plot:\n        plt.plot(precision, recall,  label=f\"{tf}\")\n        plt.title(f\"Precision-Recall Curve: {tf_name}\")\n        plt.xlabel(\"recall\")\n        plt.ylabel(\"precision\")\n    else:\n        return precision, recall, thresholds\n\n\nfor i, tf in enumerate(TF_LIST):\n    plot_prc(test_dataset, test_pred, i, tf, plot=True)\n    plt.legend()\n\n\n\n\n\ndf = pd.DataFrame(columns=[\"TF\", \"precision\", \"recall\"])\nfor i, tf in enumerate(TF_LIST):\n    precision, recall, thresholds = plot_prc(test_dataset, test_pred, i, tf, plot=False)\n    tmp_df = pd.DataFrame({\n      \"TF\": tf,\n      \"precision\": precision,\n      \"recall\": recall,\n    })\n    df = df.append(tmp_df)\ndf.to_csv(\"/home/philipp/BPNet/out/pr_curve_all_tfs_shape_model.csv\", index=False)\ndel df, tmp_df\n\n/tmp/ipykernel_1221851/1134583418.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n\n\n/tmp/ipykernel_1221851/1134583418.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n\n\n/tmp/ipykernel_1221851/1134583418.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n\n\n/tmp/ipykernel_1221851/1134583418.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n\n\n\n# loop over all four TFs:\ntrue_counts = test_dataset.tf_counts.copy()\nall_pred = test_pred.cpu().numpy().copy()\npatchcap = test_dataset.ctrl_counts.copy()\nassert np.allclose(all_pred.sum(axis=-1), 1)\n\nfor tf_index, tf in enumerate(TF_LIST):\n    patchcap_cp = patchcap.copy()\n    # subset for one tf\n    pred = all_pred[:, tf_index, :, :]\n    counts = true_counts[:, tf_index, :, :]\n    # compute auPRC fro all bins\n    all = compute_auprc_bins(counts, pred, patchcap_cp, verbose=False)\n    df = pd.DataFrame(all)\n    df.to_csv(f\"/home/philipp/BPNet/out/binsizes_auprc_{tf}_shape_model.csv\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"auprc\"], label=\"BPNet\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"random_auprc\"], label=\"random profile\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"average_auprc\"], label=\"average profile\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"patchcap_auprc\"], label=\"PATCH-CAP\")\n    plt.title(f\"{tf}\")\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "03_training.html#train-loop-1",
    "href": "03_training.html#train-loop-1",
    "title": "03 Training and Metrics",
    "section": "5.1 Train Loop",
    "text": "5.1 Train Loop\n\nmodel = BPNet(n_dil_layers=9, TF_list=TF_LIST, pred_total=True, bias_track=True).to(device)\noptimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\ntrain_loader=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\ntune_loader=DataLoader(tune_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\ntrain_shape_loss, train_count_loss, train_loss = [], [], []\ntest_shape_loss, test_count_loss, test_loss = [], [], []\n\npatience_counter = 0\n\nfor epoch in range(MAX_EPOCHS):\n\n  # test\n  test_shape_loss_epoch, test_count_loss_epoch, test_loss_epoch = [], [], []\n  with torch.no_grad():\n    for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n      one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n      shape_pred, count_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n      shape_loss = neg_log_multinomial(k_obs=tf_counts, p_pred=shape_pred, device=device)\n      if count_pred.min() < 0:\n        break\n      count_loss = ((torch.log(1 + count_pred) - torch.log(1 + tf_counts.sum(axis=-1)))**2).mean()\n      loss = shape_loss + lambda_param * count_loss\n      test_shape_loss_epoch.append(shape_loss.item())\n      test_count_loss_epoch.append(count_loss.item())\n      test_loss_epoch.append(loss.item())\n    test_shape_loss.append(sum(test_shape_loss_epoch)/len(test_shape_loss_epoch))\n    test_count_loss.append(sum(test_count_loss_epoch)/len(test_count_loss_epoch))\n    test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n  # train\n  model.train()\n  train_shape_loss_epoch, train_count_loss_epoch, train_loss_epoch = [], [], []\n  for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n    one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n    optimizer.zero_grad()\n    shape_pred, count_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n    shape_loss = neg_log_multinomial(k_obs=tf_counts, p_pred=shape_pred, device=device)\n    count_loss = ((torch.log(1 + count_pred) - torch.log(1 + tf_counts.sum(axis=-1)))**2).mean()\n    loss = shape_loss + lambda_param * count_loss\n    train_shape_loss_epoch.append(shape_loss.item())\n    train_count_loss_epoch.append(count_loss.item())\n    train_loss_epoch.append(loss.item())\n    loss.backward()\n    optimizer.step()\n  train_shape_loss.append(sum(train_shape_loss_epoch)/len(train_shape_loss_epoch))\n  train_count_loss.append(sum(train_count_loss_epoch)/len(train_count_loss_epoch))\n  train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n  if test_loss[-1] > np.array(test_loss).min():\n    patience_counter += 1\n  else:\n    patience_counter = 0\n    best_state_dict = model.state_dict()\n\n  if patience_counter == EARLY_STOP_PATIENCE:\n    break\n\nif RESTORE_BEST_WEIGHTS:\n  model.load_state_dict(best_state_dict)"
  },
  {
    "objectID": "03_training.html#train-and-tune-loss-1",
    "href": "03_training.html#train-and-tune-loss-1",
    "title": "03 Training and Metrics",
    "section": "5.2 Train and Tune Loss",
    "text": "5.2 Train and Tune Loss\n\ndf = pd.DataFrame({\"epoch\": np.arange(1, epoch+2), \"train_shape\": train_shape_loss, \"test_shape\": test_shape_loss, \"train_count\": train_count_loss, \"test_count\": test_count_loss, \"train\": train_loss, \"test\": test_loss})\ndf.to_csv(\"/home/philipp/BPNet/out/shape_counts_loss.csv\")\nfig, axis = plt.subplots(1, 3, figsize=(12, 3))\naxis[0].plot(np.arange(1, epoch+2), np.array(train_shape_loss), label=\"train\")\naxis[0].plot(np.arange(1, epoch+2), np.array(test_shape_loss), label=\"test\")\naxis[0].set_xlabel(\"Epoch\")\naxis[0].set_ylabel(\"Loss\")\naxis[0].set_title(\"Shape Loss\")\n\naxis[1].plot(np.arange(1, epoch+2), np.array(train_count_loss), label=\"train\")\naxis[1].plot(np.arange(1, epoch+2), np.array(test_count_loss), label=\"test\")\naxis[1].set_xlabel(\"Epoch\")\naxis[1].set_ylabel(\"Loss\")\naxis[1].set_title(\"Count Loss\")\n\naxis[2].plot(np.arange(1, epoch+2), np.array(train_loss), label=\"train\")\naxis[2].plot(np.arange(1, epoch+2), np.array(test_loss), label=\"test\")\naxis[2].set_xlabel(\"Epoch\")\naxis[2].set_ylabel(\"Loss\")\naxis[2].set_title(\"Combined Loss\")\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "03_training.html#save-model-1",
    "href": "03_training.html#save-model-1",
    "title": "03 Training and Metrics",
    "section": "5.3 Save Model",
    "text": "5.3 Save Model\n\ntorch.save(obj=model, f=\"/home/philipp/BPNet/trained_models/all_tfs_model.pt\")"
  },
  {
    "objectID": "03_training.html#evaluation-1",
    "href": "03_training.html#evaluation-1",
    "title": "03 Training and Metrics",
    "section": "5.4 Evaluation",
    "text": "5.4 Evaluation\n\nmodel = torch.load(\"/home/philipp/BPNet/trained_models/all_tfs_model.pt\")\n\n\n5.4.1 Check Examples\nPlotting the real counts and the predictions for the first batch from the tune dataset.\n\ntune_loader=DataLoader(tune_dataset, batch_size=10, shuffle=False, num_workers=0, pin_memory=True)\none_hot, tf_counts, ctrl_counts, ctrl_smooth = next(tune_loader.__iter__())\none_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\nprofile_pred, _ = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\nprofile_pred = profile_pred.to(\"cpu\").detach().numpy()\ntf_counts = tf_counts.to(\"cpu\").detach().numpy()\nscaled_pred = profile_pred * tf_counts.sum(axis=-1)[:,:,:,None]\nlw = 0.8\n\n\n5.4.1.1 Specific Sequence\n\ntmp_df = test_dataset.region_info.copy().reset_index()\nidx = tmp_df.loc[(tmp_df.seqnames==\"chr1\") & (tmp_df.start > 180924752-1000) & (tmp_df.end < 180925152+1000) & (tmp_df.TF == \"Sox2\")].index.to_numpy()[0]\nprint(tmp_df.iloc[idx])\n\nshape_pred, count_pred = model.forward(torch.from_numpy(test_dataset.one_hot_seqs[idx:idx+1, ]).to(device), torch.from_numpy(test_dataset.ctrl_counts[idx:idx+1, ]).to(device), torch.from_numpy(test_dataset.ctrl_counts_smooth[idx:idx+1, ]).to(device))\n\nshape_pred = shape_pred.cpu().detach().numpy()\n\ndf = pd.DataFrame(columns=[\"position\", \"TF\", \"strand\", \"kind\", \"value\"])\n\nfor data, kind in zip([test_dataset.tf_counts[idx], shape_pred[0]], [\"counts\", \"prediction\"]):\n  for i, tf in enumerate(TF_LIST):\n    for j, strand in enumerate([\"pos\", \"neg\"]):\n      tmp_df = pd.DataFrame({\"position\": np.arange(1000), \"TF\": tf, \"strand\": strand, \"kind\": kind, \"value\": data[i, j]})\n      df = df.append(tmp_df)\ndf.to_csv(\"/home/philipp/BPNet/out/example_shape_prediction.csv\")\n\nindex                                97\nseqnames                           chr1\nstart                         180924435\nend                           180925434\nwidth                              1000\nstrand                                *\nTF                                 Sox2\nset                                test\nname                                NaN\nscore                              1000\nsignalValue                    17.39712\npValue                        441.17929\nqValue                        436.06531\npeak                                404\nRegion         chr1:180924435-180925434\nName: 19, dtype: object\n\n\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n/tmp/ipykernel_1221851/1462060750.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n\n\n\n\n5.4.1.2 Nanog\n\ntf = 0\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.3 Klf4\n\ntf = 1\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.4 Oct4\n\ntf = 2\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.5 Sox2\n\ntf = 3\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Precision and Recall\n\ntest_pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\ntest_count_pred = torch.zeros(test_dataset.tf_counts.shape[0:3], dtype=torch.float32).to(device)\nwith torch.no_grad():\n    for batch_idx, data in enumerate(test_dataloader):\n        #print(batch_idx, batch_idx + 100)#, data)\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        profile_pred, count_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        #print(profile_pred.shape)\n        start = batch_idx*BATCH_SIZE\n        end = (batch_idx+1)*BATCH_SIZE if (batch_idx+1)*BATCH_SIZE < test_dataset.tf_counts.shape[0] else test_dataset.tf_counts.shape[0]\n        test_pred[start:end] = profile_pred\n        test_count_pred[start:end] = count_pred\n\n\ndef plot_prc(test_dataset, test_pred, tf_index, tf_name, plot = True):\n    true_counts = test_dataset.tf_counts.copy()\n    #subset for one tf\n    tf_counts = true_counts[:, tf_index, :, :]\n    test_pred = test_pred.cpu().numpy().copy()\n    assert np.allclose(test_pred.sum(axis=-1), 1)\n    # subset for one tf\n    tf_pred = test_pred[:, tf_index, :, :]\n    binary_labels, pred_subset, random = binary_labels_from_counts(tf_counts, tf_pred, verbose=False)\n    precision, recall, thresholds = skm.precision_recall_curve(binary_labels, pred_subset)\n    if plot:\n        plt.plot(recall, precision,  label=f\"{tf}\")\n        plt.title(f\"Precision-Recall Curve: {tf_name}\")\n        plt.xlabel(\"recall\")\n        plt.ylabel(\"precision\")\n    else:\n        return precision, recall, thresholds\n\n\nfor i, tf in enumerate(TF_LIST):\n    plot_prc(test_dataset, test_pred, i, tf, plot=True)\n    plt.legend()\n\n\n\n\n\ndf = pd.DataFrame(columns=[\"TF\", \"precision\", \"recall\"])\nfor i, tf in enumerate(TF_LIST):\n    precision, recall, thresholds = plot_prc(test_dataset, test_pred, i, tf, plot=False)\n    tmp_df = pd.DataFrame({\n      \"TF\": tf,\n      \"precision\": precision,\n      \"recall\": recall,\n    })\n    df = df.append(tmp_df)\ndf.to_csv(\"/home/philipp/BPNet/out/pr_curve_all_tfs_count_model.csv\", index=False)\ndel df, tmp_df\n\n/tmp/ipykernel_1221851/4023111080.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n\n\n/tmp/ipykernel_1221851/4023111080.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n\n\n/tmp/ipykernel_1221851/4023111080.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n\n\n/tmp/ipykernel_1221851/4023111080.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df.append(tmp_df)\n\n\n\n# loop over all four TFs:\ntrue_counts = test_dataset.tf_counts.copy()\nall_pred = test_pred.cpu().numpy().copy()\npatchcap = test_dataset.ctrl_counts.copy()\nassert np.allclose(all_pred.sum(axis=-1), 1)\n\nfor tf_index, tf in enumerate(TF_LIST):\n    patchcap_cp = patchcap.copy()\n    # subset for one tf\n    pred = all_pred[:, tf_index, :, :]\n    counts = true_counts[:, tf_index, :, :]\n    # compute auPRC fro all bins\n    all = compute_auprc_bins(counts, pred, patchcap_cp, verbose=False)\n    df = pd.DataFrame(all)\n    df.to_csv(f\"/home/philipp/BPNet/out/binsizes_auprc_{tf}_count_model.csv\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"auprc\"], label=\"BPNet\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"random_auprc\"], label=\"random profile\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"average_auprc\"], label=\"average profile\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"patchcap_auprc\"], label=\"PATCH-CAP\")\n    plt.title(f\"{tf}\")\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.3 MSE and R2\n\ntrue_total_counts = test_dataset.tf_counts.sum(axis=-1).copy()\npred_total_counts = test_count_pred.cpu().detach().numpy()\ntf_means = true_total_counts.mean(axis=0)\n\ndf = pd.DataFrame(columns=[\"TF\", \"mse\", \"tss\", \"rss\", \"r2\"])\nfor i, tf in enumerate(TF_LIST):\n  mse = ((np.log1p(true_total_counts[:, i]) - np.log1p(pred_total_counts[:, i]))**2).mean()\n  tss = ((true_total_counts[:, i] - tf_means[None, i])**2).sum()\n  rss = ((true_total_counts[:, i] - pred_total_counts[:, i])**2).sum()\n  r2 = 1 - rss/tss\n  tmp_df = pd.DataFrame({\"TF\": tf, \"mse\": mse, \"tss\": tss, \"rss\": rss, \"r2\": r2}, index=[0])\n  df = pd.concat([df, tmp_df], ignore_index=True, axis=0)\ndf.to_csv(\"/home/philipp/BPNet/out/count_stats.csv\", index=False)"
  },
  {
    "objectID": "04_architecture_comparison.html",
    "href": "04_architecture_comparison.html",
    "title": "04 Architecture Comparisons",
    "section": "",
    "text": "TF_LIST = [\"Nanog\", \"Klf4\", \"Oct4\", \"Sox2\"]\nINPUT_DIR = \"/home/philipp/BPNet/input/\"\nBATCH_SIZE = 64\nALPHA = 10\nMAX_EPOCHS = 100  \nEARLY_STOP_PATIENCE = 4\nRESTORE_BEST_WEIGHTS = True\nplot_while_train = False\nretrain_conv_layers = False\nretrain_channel = False\nretrain_kern_size = False"
  },
  {
    "objectID": "04_architecture_comparison.html#training-only-shape-prediction",
    "href": "04_architecture_comparison.html#training-only-shape-prediction",
    "title": "04 Architecture Comparisons",
    "section": "4.1 Training (Only Shape Prediction)",
    "text": "4.1 Training (Only Shape Prediction)\n\nn_layers_list = np.arange(1,16)\n\nif retrain_conv_layers:\n  for n_layers in n_layers_list:\n    model = BPNet(n_dil_layers=n_layers, TF_list=TF_LIST, pred_total=False, bias_track=True).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\n    train_loader=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n    tune_loader=DataLoader(tune_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\n    train_loss, test_loss = [], []\n    patience_counter = 0\n\n    for epoch in range(MAX_EPOCHS):\n      model.train()\n      train_loss_epoch = []\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        optimizer.zero_grad()\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n        train_loss_epoch.append(loss.item())\n        loss.backward()\n        optimizer.step()\n      train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n      # evaluation part\n      test_loss_epoch = []\n      with torch.no_grad():\n          for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n              one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n              profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n              loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n              test_loss_epoch.append(loss.item())\n          test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n      if test_loss[-1] > np.array(test_loss).min():\n        patience_counter += 1\n      else:\n        patience_counter = 0\n        best_state_dict = model.state_dict()\n\n      if patience_counter == EARLY_STOP_PATIENCE:\n        break\n    \n    if RESTORE_BEST_WEIGHTS:\n      model.load_state_dict(best_state_dict)\n\n    # plot train and test loss\n    plt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\n    plt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # save the model\n    torch.save(obj=model, f=f\"/home/philipp/BPNet/trained_models/{n_layers}_dil_layers_model.pt\")"
  },
  {
    "objectID": "04_architecture_comparison.html#evaluation",
    "href": "04_architecture_comparison.html#evaluation",
    "title": "04 Architecture Comparisons",
    "section": "4.2 Evaluation",
    "text": "4.2 Evaluation\n\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=INPUT_DIR, \n                                  TF_list=TF_LIST)\ntest_dataset\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n\n\n\nsave_scores = []\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\ntrue_counts = test_dataset.tf_counts.copy()\nfor n in n_layers_list:\n  model = torch.load(f\"/home/philipp/BPNet/trained_models/{n}_dil_layers_model.pt\")\n  # make predictions\n  pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\n  with torch.no_grad():\n      for batch_idx, data in enumerate(test_dataloader):\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          pred[batch_idx, :, :, :] = profile_pred\n  all_pred = pred.cpu().numpy().copy()\n  assert np.allclose(all_pred.sum(axis=-1), 1)\n          \n  for i, tf in enumerate(TF_LIST): # loop over the four TFs\n      pred_tf = all_pred[:, i, :, :]\n      counts_tf = true_counts[:, i, :, :]\n      labels, predictions, random = binary_labels_from_counts(counts_tf, pred_tf)\n      auprc_score = skm.average_precision_score(labels, predictions)\n      save_scores.append({\"tf\": tf,\n                          \"n_layers\":n,\n                          \"auprc\": auprc_score})\n\n\ndf = pd.DataFrame(save_scores)\ndf.to_csv(\"/home/philipp/BPNet/out/dil_layers_auprc.csv\")\nsns.scatterplot(data=df, x=\"n_layers\", y=\"auprc\", hue=\"tf\", palette=color_pal)\nplt.show()"
  },
  {
    "objectID": "04_architecture_comparison.html#training-only-shape-prediction-1",
    "href": "04_architecture_comparison.html#training-only-shape-prediction-1",
    "title": "04 Architecture Comparisons",
    "section": "5.1 Training (Only Shape Prediction)",
    "text": "5.1 Training (Only Shape Prediction)\n\nn_channel_list = np.array([2, 4, 8, 16, 32, 64, 128, 256])\n\nif retrain_channel:\n  for n_channel in n_channel_list:\n    model = BPNet(n_dil_layers=9, TF_list=TF_LIST, pred_total=False, bias_track=True, conv_channels=n_channel).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\n    train_loader=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n    tune_loader=DataLoader(tune_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\n    train_loss, test_loss = [], []\n    patience_counter = 0\n\n    for epoch in range(MAX_EPOCHS):\n      model.train()\n      train_loss_epoch = []\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        optimizer.zero_grad()\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n        train_loss_epoch.append(loss.item())\n        loss.backward()\n        optimizer.step()\n      train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n      # evaluation part\n      test_loss_epoch = []\n      with torch.no_grad():\n          for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n              one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n              profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n              loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n              test_loss_epoch.append(loss.item())\n          test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n      if test_loss[-1] > np.array(test_loss).min():\n        patience_counter += 1\n      else:\n        patience_counter = 0\n        best_state_dict = model.state_dict()\n\n      if patience_counter == EARLY_STOP_PATIENCE:\n        break\n    \n    if RESTORE_BEST_WEIGHTS:\n      model.load_state_dict(best_state_dict)\n\n    # plot train and test loss\n    plt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\n    plt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # save the model\n    torch.save(obj=model, f=f\"/home/philipp/BPNet/trained_models/{n_channel}_conv_channel_model.pt\")"
  },
  {
    "objectID": "04_architecture_comparison.html#evaluation-1",
    "href": "04_architecture_comparison.html#evaluation-1",
    "title": "04 Architecture Comparisons",
    "section": "5.2 Evaluation",
    "text": "5.2 Evaluation\n\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=INPUT_DIR, \n                                  TF_list=TF_LIST)\ntest_dataset\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n\n\n\nsave_scores = []\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\ntrue_counts = test_dataset.tf_counts.copy()\nfor n in n_channel_list:\n  model = torch.load(f\"/home/philipp/BPNet/trained_models/{n}_conv_channel_model.pt\")\n  # make predictions\n  pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\n  with torch.no_grad():\n      for batch_idx, data in enumerate(test_dataloader):\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          pred[batch_idx, :, :, :] = profile_pred\n  all_pred = pred.cpu().numpy().copy()\n  assert np.allclose(all_pred.sum(axis=-1), 1)\n          \n  for i, tf in enumerate(TF_LIST): # loop over the four TFs\n      pred_tf = all_pred[:, i, :, :]\n      counts_tf = true_counts[:, i, :, :]\n      labels, predictions, random = binary_labels_from_counts(counts_tf, pred_tf)\n      auprc_score = skm.average_precision_score(labels, predictions)\n      save_scores.append({\"tf\": tf,\n                          \"n_channels\":n,\n                          \"auprc\": auprc_score})\n\n\ndf = pd.DataFrame(save_scores)\ndf.to_csv(\"/home/philipp/BPNet/out/conv_channel_auprc.csv\")\nsns.scatterplot(data=df, x=\"n_channels\", y=\"auprc\", hue=\"tf\", palette=color_pal)\nplt.show()"
  },
  {
    "objectID": "04_architecture_comparison.html#training-only-shape-prediction-2",
    "href": "04_architecture_comparison.html#training-only-shape-prediction-2",
    "title": "04 Architecture Comparisons",
    "section": "6.1 Training (Only Shape Prediction)",
    "text": "6.1 Training (Only Shape Prediction)\n\nkernel_sizes = np.array([5, 9, 13, 17, 21, 25, 29, 33, 37])\n\nif retrain_kern_size:\n  for kern_size in kernel_sizes:\n    model = BPNet(n_dil_layers=9, TF_list=TF_LIST, pred_total=False, bias_track=True, conv_channels=64, size_first_kernel=kern_size).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\n    train_loader=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n    tune_loader=DataLoader(tune_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\n    train_loss, test_loss = [], []\n    patience_counter = 0\n\n    for epoch in range(MAX_EPOCHS):\n      model.train()\n      train_loss_epoch = []\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        optimizer.zero_grad()\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n        train_loss_epoch.append(loss.item())\n        loss.backward()\n        optimizer.step()\n      train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n      # evaluation part\n      test_loss_epoch = []\n      with torch.no_grad():\n          for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n              one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n              profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n              loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n              test_loss_epoch.append(loss.item())\n          test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n      if test_loss[-1] > np.array(test_loss).min():\n        patience_counter += 1\n      else:\n        patience_counter = 0\n        best_state_dict = model.state_dict()\n\n      if patience_counter == EARLY_STOP_PATIENCE:\n        break\n    \n    if RESTORE_BEST_WEIGHTS:\n      model.load_state_dict(best_state_dict)\n\n    # plot train and test loss\n    plt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\n    plt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # save the model\n    torch.save(obj=model, f=f\"/home/philipp/BPNet/trained_models/{kern_size}_first_kern_size_model.pt\")"
  },
  {
    "objectID": "04_architecture_comparison.html#evaluation-2",
    "href": "04_architecture_comparison.html#evaluation-2",
    "title": "04 Architecture Comparisons",
    "section": "6.2 Evaluation",
    "text": "6.2 Evaluation\n\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=INPUT_DIR, \n                                  TF_list=TF_LIST)\ntest_dataset\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n\n\n\nsave_scores = []\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\ntrue_counts = test_dataset.tf_counts.copy()\nfor n in kernel_sizes:\n  model = torch.load(f\"/home/philipp/BPNet/trained_models/{n}_first_kern_size_model.pt\")\n  # make predictions\n  pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\n  with torch.no_grad():\n      for batch_idx, data in enumerate(test_dataloader):\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          pred[batch_idx, :, :, :] = profile_pred\n  all_pred = pred.cpu().numpy().copy()\n  assert np.allclose(all_pred.sum(axis=-1), 1)\n          \n  for i, tf in enumerate(TF_LIST): # loop over the four TFs\n      pred_tf = all_pred[:, i, :, :]\n      counts_tf = true_counts[:, i, :, :]\n      labels, predictions, random = binary_labels_from_counts(counts_tf, pred_tf)\n      auprc_score = skm.average_precision_score(labels, predictions)\n      save_scores.append({\"tf\": tf,\n                          \"first_kern_size\": n,\n                          \"auprc\": auprc_score})\n\n\ndf = pd.DataFrame(save_scores)\ndf.to_csv(\"/home/philipp/BPNet/out/first_kern_size_auprc.csv\")\nsns.scatterplot(data=df, x=\"first_kern_size\", y=\"auprc\", hue=\"tf\", palette=color_pal)\nplt.show()"
  }
]