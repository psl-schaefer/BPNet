[
  {
    "objectID": "code/07_mutagenesis.html",
    "href": "code/07_mutagenesis.html",
    "title": "Mutagenesis",
    "section": "",
    "text": "1 Description\n\nHere we perform in-silico mutagenesis experiments\n\n\n\n2 Libraries\n\n\nCode\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom captum.attr import DeepLift\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport modisco.visualization\nfrom modisco.visualization import viz_sequence\n\nfrom src.config import conf_dict\nfrom src.architectures import *\nfrom src.utils import * \nfrom src.loss import *\nfrom src.metrics import *\nfrom src.DeepLiftUtils import *\n\n# dark mode doesn't quite work for the attribution plots\n#plt.style.use('dark_background')\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n3 Data\n\n\nCode\n#conf_dict[\"tf_list\"] = [\"Nanog\"]\n#conf_dict[\"batch_size\"] = 248\n#conf_dict[\"max_epochs\"] = 25\n#conf_dict[\"early_stop_patience\"] = 4\n#conf_dict[\"restore_best_weights\"] = True\n\nPRC_DIR = Path(\"..\") / \"prc\"\nMODELS_DIR = Path(\"..\") / \"trained_models\"\nOUTPUT_DIR = Path(\"..\") / \"prc\" / \"mutagenesis\"\nOUTPUT_DIR.mkdir(exist_ok=True, parents=True)\nFIG_DIR = Path(\"..\") / \"figures\" / \"mutagenesis\"\nFIG_DIR.mkdir(exist_ok=True, parents=True)\n\n\n\n\nCode\nif torch.cuda.is_available():\n  device = \"cuda\"\nelif torch.backends.mps.is_available():\n  device = torch.device(\"mps\")\nelse:\n  decive = \"cpu\"\nprint(f\"Using {device} device\")\n\n\nUsing mps device\n\n\n\n\nCode\ntrain_dataset = ChIP_Nexus_Dataset(set_name=\"train\", \n                                   input_dir=PRC_DIR, \n                                   TF_list=conf_dict[\"tf_list\"])\ntrain_dataset\n\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 93904\n\n\n\n\nCode\ntune_dataset = ChIP_Nexus_Dataset(set_name=\"tune\", \n                                  input_dir=PRC_DIR, \n                                  TF_list=conf_dict[\"tf_list\"])\ntune_dataset\n\n\nChIP_Nexus_Dataset\nSet: tune\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 29277\n\n\n\n\nCode\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=PRC_DIR, \n                                  TF_list=conf_dict[\"tf_list\"])\ntest_dataset\n\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n\n\n\n\nCode\nmodel = torch.load(MODELS_DIR / \"all_tfs_model.pt\")\n\n\n/var/folders/n8/xnf8qmpj41795tf723xy8mqm0000gn/T/ipykernel_97745/1827116589.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / \"all_tfs_model.pt\")\n\n\n\n\nCode\ndl = DeepLift(model)\n\n\n\n\nCode\nseqname = \"chr15\"\nstart = 80618982\nend = 80619381\nlength = end - start\n\n\n\n\nCode\ncenter = start + 200\n\na = start + 193\nb = end - 192\nprint(a, b, b-a)\n\n\n80619175 80619189 14\n\n\n\n\nCode\ndef remove_motif(seqname, start, end, dataset, dl, tf_list, device, output_dir, suffix, plot=True, extend_left=23, extend_right=23, figsize1=(20,2), figsize2=(10,2)):\n  # select sequence of interest\n  tmp_df, idx, dist_start, one_hot, baseline, bias_raw, bias_smooth, tf_counts = get_seq_oi(seqname, start, end, dataset, device)\n  width = end - start\n  center = start + 200\n\n  #shuffle region of interest\n  mut_one_hot = one_hot.clone().cpu().numpy()\n  roi = mutate_sequence(mut_one_hot[:,:, dist_start:dist_start+width])\n  # insert shuffled region back in \n  mut_one_hot[:, :, dist_start:dist_start+end-start] = roi\n  mut_one_hot = torch.tensor(mut_one_hot).to(device)\n  assert torch.all(one_hot == mut_one_hot) == False\n\n  contr_list = []\n  df_list = []\n  for tf_index, tf in enumerate(conf_dict[\"tf_list\"]):\n    # compute contribution scores on original and mutated strand\n    contr = dl.attribute(inputs=one_hot, baselines=baseline, target=tf_index, \n            additional_forward_args=(bias_raw, bias_smooth, True)).detach().cpu().numpy()\n    mut_contr = dl.attribute(inputs=mut_one_hot, baselines = baseline, target=tf_index,\n            additional_forward_args=(bias_raw, bias_smooth, True)).detach().cpu().numpy()\n\n    # make predictions on original and mutated strand\n    pred, _ = model.forward(one_hot, bias_raw, bias_smooth, interpretation=False)\n    pred = pred.detach().cpu().numpy().squeeze()\n    mut_pred,_ = model.forward(mut_one_hot, bias_raw, bias_smooth, interpretation=False)\n    mut_pred = mut_pred.detach().cpu().numpy().squeeze()\n\n    # scale predictions with tf_counts for plotting\n    #  print(pred.shape, tf_counts.shape)\n    pred = pred * tf_counts.sum(axis=-1, keepdims=True)\n    mut_pred = mut_pred * tf_counts.sum(axis=-1, keepdims=True)\n\n    tf_df = pd.DataFrame({\"pos\": np.arange(1000), \"TF\": tf, \"pos_values\": pred[tf_index, 0,:], \"neg_values\": pred[tf_index, 1,:], \"mut_pos_values\" : mut_pred[tf_index, 0, :], \"mut_neg_values\" : mut_pred[tf_index, 1,:], \"tf_counts_pos\": tf_counts[tf_index, 0, :], \"tf_counts_neg\": tf_counts[tf_index, 1, :]})\n    df_list.append(tf_df)\n\n    if plot:\n      plot_weights(contr,\n      fontsizes=[20,15,15],\n      title = f\"{tf} - Original sequence\", \n      xlabel=f\"{tmp_df.seqnames[idx]}: {tmp_df.start[idx]}-{tmp_df.end[idx]}\", \n      ylabel=\"DeepLift contribution scores\",\n      subticks_frequency=20, figsize=figsize1)\n\n      # zoomed into motif region\n      plot_weights(contr[:, :,dist_start-extend_left : (dist_start + width + extend_right)],\n      fontsizes=[20,15,15],\n      title = f\"{tf} - Original motif\", \n      xlabel=f\"{seqname}: {start}-{end}\",#, ({dist_start} - {dist_start + width})\", \n      ylabel=\"DeepLift contribution scores\",\n      subticks_frequency=10, figsize=figsize2)\n      plt.savefig(output_dir / f\"{tf}_{seqname}_{start}_{end}_original_{suffix}.pdf\")  \n\n      # entire mutated sequence\n      plot_weights(mut_contr,\n      fontsizes=[20,15,15],\n      title = f\"{tf} - Mutated sequence\", \n      xlabel=f\"{tmp_df.seqnames[idx]}: {tmp_df.start[idx]}-{tmp_df.end[idx]}\", \n      ylabel=\"DeepLift contribution scores\",\n      subticks_frequency=20, figsize=figsize1)\n\n      # zoomed in mutated sequence\n      plot_weights(mut_contr[:, :,dist_start-extend_left : (dist_start + width + extend_right)],\n      fontsizes=[20,15,15],\n      title = f\"{tf} - Mutated motif\", \n      xlabel=f\"{seqname}: {start}-{end}\",#, ({dist_start} - {dist_start + width})\", \n      ylabel=\"DeepLift contribution scores\",\n      subticks_frequency=10, figsize=figsize2)\n      plt.savefig(f\"{output_dir}{tf}_{seqname}_{start}_{end}_mutated_{suffix}.pdf\")  \n      \n      #fig = plt.figure(figsize=(10,4))\n      plt.plot(-tf_counts[tf_index, 1, :], color=\"green\", linewidth=0.8)\n      plt.plot()\n      fig, axis = plt.subplots(2,1,figsize=(10,8))\n      axis[0].plot(tf_counts[tf_index, 0, :], label=\"true counts\", color=\"green\", linewidth=0.8)\n      axis[0].plot(-tf_counts[tf_index, 1, :], color=\"green\", linewidth=0.8)\n      axis[0].plot(pred[tf_index, 0, :], label=\"pred\", color=\"blue\", linewidth=0.8)\n      axis[0].plot(-pred[tf_index, 1, :], color=\"blue\", linewidth=0.8)   \n      axis[0].set_xlabel(\"bp\")\n      axis[0].set_ylabel(\"Read counts\")\n      axis[1].plot(tf_counts[tf_index, 1, :], color=\"green\", linewidth=0.8)\n      axis[1].plot(-tf_counts[tf_index, 1, :], color=\"green\", linewidth=0.8)\n      axis[1].plot(mut_pred[tf_index, 0, :], label=\"pred mutated strand\", color=\"darkred\", linewidth=0.8)\n      axis[1].plot(-mut_pred[tf_index, 1, :], color=\"darkred\", linewidth=0.8)      \n      axis[1].set_xlabel(\"bp\")\n      axis[0].legend()\n      axis[1].legend()\n      plt.show()\n  plot_df = pd.concat(df_list)\n\n  return contr, mut_contr, dist_start, plot_df\n\n\n\n\n4 Remove entire motif\n\n\nCode\n_, _, dist_start, plot_df = remove_motif(seqname, start=a, end=b, dataset=train_dataset,  dl=dl, suffix=\"remove\", output_dir=FIG_DIR, tf_list=conf_dict[\"tf_list\"], device=device, figsize1=(30,2), figsize2=(10,1.5))\nplot_df.to_csv(OUTPUT_DIR / \"remove_motif.csv\")\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\noutput_dir = py$FIG_DIR\ncolors = c(\"Oct4\" = \"#CD5C5C\",\n           \"Sox2\" = \"#849EEB\",\n           \"Nanog\" = \"#FFE03F\",\n           \"Klf4\" = \"#92C592\",\n           \"patchcap\" = \"#827F81\")\ndf = read.csv(file.path(py$OUTPUR_DIR, \"remove_motif.csv\"))\ndf %&gt;%\n  ggplot() +\n  geom_line(aes(x=pos, y=tf_counts_pos, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=-tf_counts_neg, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=pos_values, col=TF), size=.7) +\n  geom_line(aes(x=pos, y=-neg_values, col=TF), size=.7) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  scale_color_manual(values=colors) + \n  #scale_alpha_manual(values=c(\"pos\" = 1, \"neg\" = 1)) +\n  theme_bw()\nggsave(paste0(output_dir, \"chr4_55475545_55475604_remove_pred.pdf\"))\n\ndf %&gt;%\n  ggplot() +\n  geom_line(aes(x=pos, y=tf_counts_pos, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=-tf_counts_neg, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=mut_pos_values, col=TF), size=.7) +\n  geom_line(aes(x=pos, y=-mut_neg_values, col=TF), size=.7) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  scale_color_manual(values=colors) + \n  #scale_alpha_manual(values=c(\"pos\" = 1, \"neg\" = 1)) +\n  theme_bw()\nggsave(paste0(output_dir, \"chr4_55475545_55475604_remove_mutpred.pdf\"))\n  #ggdark::dark_theme_linedraw()\n#ggsave(paste0(output_dir, \"chr4_55475545_55475604_zoomed_traces.pdf\"))\n\n\n5 Remove Oct4 part\n\n\nCode\n_, _, dist_start, plot_df = remove_motif(seqname, start=a, end=a+8, dataset=train_dataset, suffix=\"remove_oct4\", output_dir=FIG_DIR,  dl=dl, tf_list=conf_dict[\"tf_list\"], device=device, extend_left=23, extend_right=23+6, figsize1=(30,2), figsize2=(10, 1.5))\nplot_df.to_csv(OUTPUT_DIR / \"remove_oct4.csv\")\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\noutput_dir = py$FIG_DIR\ncolors = c(\"Oct4\" = \"#CD5C5C\",\n           \"Sox2\" = \"#849EEB\",\n           \"Nanog\" = \"#FFE03F\",\n           \"Klf4\" = \"#92C592\",\n           \"patchcap\" = \"#827F81\")\ndf = read.csv(file.path(py$OUTPUR_DIR, \"remove_oct4.csv\"))\ndf %&gt;%\n  ggplot() +\n  geom_line(aes(x=pos, y=tf_counts_pos, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=-tf_counts_neg, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=pos_values, col=TF), size=.7) +\n  geom_line(aes(x=pos, y=-neg_values, col=TF), size=.7) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  scale_color_manual(values=colors) + \n  #scale_alpha_manual(values=c(\"pos\" = 1, \"neg\" = 1)) +\n  theme_bw()\nggsave(paste0(output_dir, \"chr4_55475545_55475604_remove_oct4_pred.pdf\"))\n\ndf %&gt;%\n  ggplot() +\n  geom_line(aes(x=pos, y=tf_counts_pos, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=-tf_counts_neg, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=mut_pos_values, col=TF), size=.7) +\n  geom_line(aes(x=pos, y=-mut_neg_values, col=TF), size=.7) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  scale_color_manual(values=colors) + \n  #scale_alpha_manual(values=c(\"pos\" = 1, \"neg\" = 1)) +\n  theme_bw()\nggsave(paste0(output_dir, \"chr4_55475545_55475604_remove_oct4_mutpred.pdf\"))\n  #ggdark::dark_theme_linedraw()\n#ggsave(paste0(output_dir, \"chr4_55475545_55475604_zoomed_traces.pdf\"))\n\n\n6 Remove Sox2 part\n\n\nCode\n_, _, dist_start, plot_df = remove_motif(seqname, start=a+8, end=b, dataset=train_dataset,  suffix=\"remove_sox2\", output_dir=FIG_DIR, dl=dl, tf_list=conf_dict[\"tf_list\"], device=device, figsize1=(30,2), figsize2=(10,1.5))\nplot_df.to_csv(OUTPUT_DIR / \"remove_sox2.csv\")\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\noutput_dir = py$FIG_DIR\ncolors = c(\"Oct4\" = \"#CD5C5C\",\n           \"Sox2\" = \"#849EEB\",\n           \"Nanog\" = \"#FFE03F\",\n           \"Klf4\" = \"#92C592\",\n           \"patchcap\" = \"#827F81\")\ndf = read.csv(file.path(py$OUTPUR_DIR, \"remove_sox2.csv\"))\ndf %&gt;%\n  ggplot() +\n  geom_line(aes(x=pos, y=tf_counts_pos, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=-tf_counts_neg, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=pos_values, col=TF), size=.7) +\n  geom_line(aes(x=pos, y=-neg_values, col=TF), size=.7) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  scale_color_manual(values=colors) + \n  #scale_alpha_manual(values=c(\"pos\" = 1, \"neg\" = 1)) +\n  theme_bw()\nggsave(paste0(output_dir, \"chr4_55475545_55475604_remove_sox2_pred.pdf\"))\n\ndf %&gt;%\n  ggplot() +\n  geom_line(aes(x=pos, y=tf_counts_pos, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=-tf_counts_neg, col=\"black\"))+#, alpha=0.8)) +\n  geom_line(aes(x=pos, y=mut_pos_values, col=TF), size=.7) +\n  geom_line(aes(x=pos, y=-mut_neg_values, col=TF), size=.7) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  scale_color_manual(values=colors) + \n  #scale_alpha_manual(values=c(\"pos\" = 1, \"neg\" = 1)) +\n  theme_bw()\nggsave(paste0(output_dir, \"chr4_55475545_55475604_remove_sox2_mutpred.pdf\"))\n  #ggdark::dark_theme_linedraw()\n#ggsave(paste0(output_dir, \"chr4_55475545_55475604_zoomed_traces.pdf\"))\n\n\n7 Move Sox2 part somewhere else\n\n\nCode\n# here I move the second motif(Sox2) d bps to the right\ndef move_motifs_apart(seqname, start, end, dataset, DeepLiftModel, distance, plot=True, figsize1=(20,2), figsize2=(10,2)):\n  # select sequence of interest\n  tmp_df, idx, dist_start, one_hot, baseline, bias_raw, bias_smooth, tf_counts = get_seq_oi(seqname, start, end, dataset, device)\n  width = end - start\n\n  #shuffle region of interest\n  mut_one_hot = one_hot.clone().cpu().numpy()\n  roi = mutate_sequence(mut_one_hot[:, :, dist_start:dist_start+width])\n  mut_one_hot[:, :, dist_start:dist_start+end-start] = roi\n  # add motif at new location 10kb to the right\n  mut_one_hot[:, :, dist_start+distance:dist_start+width+distance] = one_hot[:, :, dist_start:dist_start+width].detach().cpu().numpy()\n  print(f\"New location of motif at: {distance} - {distance+width}\")\n  mut_one_hot = torch.tensor(mut_one_hot).to(device)\n  assert torch.all(one_hot == mut_one_hot) == False\n\n\n  contr_list = []\n  for tf_index, tf in enumerate(conf_dict[\"tf_list\"]):\n    # compute contribution scores on original and mutated strand\n    contr = dl.attribute(inputs=one_hot, baselines=baseline, target=tf_index, additional_forward_args=(bias_raw, bias_smooth, True)).detach().cpu().numpy()\n    mut_contr = dl.attribute(inputs=mut_one_hot, baselines = baseline, target=tf_index, additional_forward_args=(bias_raw, bias_smooth, True)).detach().cpu().numpy()\n\n    # make predictions on original and mutated strand\n    pred, _ = model.forward(one_hot, bias_raw, bias_smooth, interpretation=False)\n    pred = pred.detach().cpu().numpy().squeeze()\n    mut_pred,_ = model.forward(mut_one_hot, bias_raw, bias_smooth, interpretation=False)\n    mut_pred = mut_pred.detach().cpu().numpy().squeeze()\n\n    # scale predictions with tf_counts for plotting\n    pred = pred * tf_counts.sum(axis=-1, keepdims=True)\n    mut_pred = mut_pred * tf_counts.sum(axis=-1, keepdims=True)\n    if plot:\n      plot_weights(contr,\n      fontsizes=[20,15,15],\n      title = f\"{tf} - Original sequence\", \n      xlabel=f\"{tmp_df.seqnames[idx]}: {tmp_df.start[idx]}-{tmp_df.end[idx]}\", \n      ylabel=\"DeepLift contribution scores\",\n      subticks_frequency=20, figsize=figsize1)\n\n      # zoomed into motif region\n      plot_weights(contr[:, :,dist_start : (dist_start + width)],\n      fontsizes=[20,15,15],\n      title = f\"{tf} - Original motif\", \n      xlabel=f\"{seqname}: {start}-{end}, ({dist_start} - {dist_start + width})\", \n      ylabel=\"DeepLift contribution scores\",\n      subticks_frequency=1, figsize=figsize2)\n\n      # entire mutated sequence\n      plot_weights(mut_contr,\n      fontsizes=[20,15,15],\n      title = f\"{tf} - Motif moved to {distance} - {distance+width}\", \n      xlabel=f\"{tmp_df.seqnames[idx]}: {tmp_df.start[idx]}-{tmp_df.end[idx]}\", \n      ylabel=\"DeepLift contribution scores\",\n      subticks_frequency=20, figsize=figsize1)\n\n      # zoomed in mutated sequence\n      plot_weights(mut_contr[:, :,dist_start : (dist_start + width)],\n      fontsizes=[20,15,15],\n      title = f\"{tf} - Mutated motif\", \n      xlabel=f\"{seqname}: {start}-{end}, ({dist_start} - {dist_start + width})\", \n      ylabel=\"DeepLift contribution scores\",\n      subticks_frequency=1, figsize=figsize2)\n\n      # zoomed in new location of motif\n      # zoomed in mutated sequence\n      plot_weights(mut_contr[:, :,dist_start+distance:dist_start+width+distance],\n      fontsizes=[20,15,15],\n      title = f\"{tf} - Motif at new position\", \n      xlabel=f\"{seqname}: {start+distance}-{end+distance}, ({dist_start+distance} - {dist_start+distance + width})\", \n      ylabel=\"DeepLift contribution scores\",\n      subticks_frequency=1, figsize=figsize2)\n\n\n      #viz_sequence.plot_weights(contr.detach().cpu().numpy(), subticks_frequency=10, figsize=(25, 2))\n      #print(f\"coordinates: {seqname}:{start}-{end}, ({dist_start} - {dist_start + end - start})\")\n      #viz_sequence.plot_weights(contr.detach().cpu().numpy()[:, :,dist_start : (dist_start + end - start)], subticks_frequency=5, figsize=(10,2)) \n      #print(f\"motif moved:\")\n      #viz_sequence.plot_weights(mut_contr.detach().cpu().numpy(), subticks_frequency=10, figsize=(25, 2))\n      #print(f\"coordinates: {seqname}:{start}-{end}, ({dist_start} - {dist_start + end - start})\")\n      #viz_sequence.plot_weights(mut_contr.detach().cpu().numpy()[:, :,dist_start : (dist_start + end - start)], subticks_frequency=5, figsize=(10,2)) \n\n      fig, axis = plt.subplots(2,2,figsize=(12,12))\n      axis[0, 0].plot(tf_counts[tf_index, 0, :], label=\"true counts\", color=\"green\", linewidth=0.8)\n      axis[0, 0].plot(-tf_counts[tf_index, 1, :], color=\"green\", linewidth=0.8)\n      axis[0, 0].plot(pred[tf_index, 0, :], label=\"pred\", color=\"blue\", linewidth=0.8)\n      axis[0, 0].plot(-pred[tf_index, 1, :], color=\"blue\", linewidth=0.8)   \n      axis[0, 0].set_xlabel(\"bp\")\n      axis[0, 0].set_ylabel(\"Read counts\")\n      axis[0, 1].plot(pred[tf_index, 0, :], label=\"pred\", color=\"blue\", linewidth=0.8)\n      axis[0, 1].plot(-pred[tf_index, 1, :], color=\"blue\", linewidth=0.8)\n      axis[0, 1].set_xlabel(\"bp\")\n      axis[0, 1].set_ylabel(\"Predicted probabilitiy * total counts\")\n      axis[1, 0].plot(tf_counts[tf_index, 0, :], label=\"true counts\", color=\"green\", linewidth=0.8)\n      axis[1, 0].plot(-tf_counts[tf_index, 1, :], color=\"green\", linewidth=0.8)\n      axis[1, 0].plot(mut_pred[tf_index, 0, :], label=\"pred mutated strand\", color=\"darkred\", linewidth=0.8)\n      axis[1, 0].plot(-mut_pred[tf_index, 1, :], color=\"darkred\", linewidth=0.8)      \n      axis[1, 0].set_xlabel(\"bp\")\n      axis[1, 0].set_ylabel(\"Read counts\")\n      axis[1, 1].plot(mut_pred[tf_index, 0, :], label=\"pred mutated strand\", color=\"darkred\", linewidth=0.8)\n      axis[1, 1].plot(-mut_pred[tf_index, 1, :], color=\"darkred\", linewidth=0.8)\n      axis[1, 1].set_xlabel(\"bp\")\n      axis[1, 1].set_ylabel(\"Predicted probabilitiy * total counts\")\n      axis[0, 0].legend()\n      axis[1, 0].legend()\n      axis[0, 1].legend()\n      axis[1,1].legend()\n      plt.show()\n\n\n  return contr, mut_contr, dist_start\n\n\n\n\nCode\nx, y, dist_start = move_motifs_apart(seqname, a+8, b, train_dataset, dl, 10, plot=True)\n\n\nNew location of motif at: 10 - 16\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn("
  },
  {
    "objectID": "code/05_architecture_comparison.html",
    "href": "code/05_architecture_comparison.html",
    "title": "Architecture Comparisons",
    "section": "",
    "text": "Code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport sklearn.metrics as skm\nimport pandas as pd\nimport seaborn as sns\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom src.config import conf_dict\nfrom src.architectures import BPNet\nfrom src.utils import ChIP_Nexus_Dataset, dummy_shape_predictions, dummy_total_counts_predictions\nfrom src.loss import neg_log_multinomial\nfrom src.metrics import permute_array, bin_max_values, bin_counts_amb, binary_labels_from_counts, compute_auprc_bins\n\ncolor_pal = {\"Oct4\": \"#CD5C5C\", \"Sox2\": \"#849EEB\", \"Nanog\": \"#FFE03F\", \"Klf4\": \"#92C592\", \"patchcap\": \"#827F81\"}\nplt.style.use('dark_background')\n\n\n\n\nCode\nif torch.cuda.is_available():\n  device = \"cuda\"\nelif torch.backends.mps.is_available():\n  device = torch.device(\"mps\")\nelse:\n  decive = \"cpu\"\nprint(f\"Using {device} device\")\n\n\nUsing cuda device"
  },
  {
    "objectID": "code/05_architecture_comparison.html#training-only-shape-prediction",
    "href": "code/05_architecture_comparison.html#training-only-shape-prediction",
    "title": "Architecture Comparisons",
    "section": "4.1 Training (Only Shape Prediction)",
    "text": "4.1 Training (Only Shape Prediction)\n\n\nCode\nn_layers_list = np.arange(1,16)\n\nif retrain_conv_layers:\n  for n_layers in n_layers_list:\n    model = BPNet(n_dil_layers=n_layers, TF_list=conf_dict[\"tf_list\"], pred_total=False, bias_track=True).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\n    train_loader=DataLoader(train_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n    tune_loader=DataLoader(tune_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n\n    train_loss, test_loss = [], []\n    patience_counter = 0\n\n    for epoch in range(conf_dict[\"max_epochs\"]):\n      model.train()\n      train_loss_epoch = []\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        optimizer.zero_grad()\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n        train_loss_epoch.append(loss.item())\n        loss.backward()\n        optimizer.step()\n      train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n      # evaluation part\n      test_loss_epoch = []\n      with torch.no_grad():\n          for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n              one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n              profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n              loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n              test_loss_epoch.append(loss.item())\n          test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n      if test_loss[-1] &gt; np.array(test_loss).min():\n        patience_counter += 1\n      else:\n        patience_counter = 0\n        best_state_dict = model.state_dict()\n\n      if patience_counter == conf_dict[\"early_stop_patience\"]:\n        break\n    \n    if conf_dict[\"restore_best_weights\"]:\n      model.load_state_dict(best_state_dict)\n\n    # plot train and test loss\n    plt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\n    plt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # save the model\n    torch.save(obj=model, f=MODELS_DIR / f\"{n_layers}_dil_layers_model.pt\")"
  },
  {
    "objectID": "code/05_architecture_comparison.html#evaluation",
    "href": "code/05_architecture_comparison.html#evaluation",
    "title": "Architecture Comparisons",
    "section": "4.2 Evaluation",
    "text": "4.2 Evaluation\n\n\nCode\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=PRC_DIR, \n                                  TF_list=conf_dict[\"tf_list\"])\ntest_dataset\n\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n\n\n\n\nCode\nsave_scores = []\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\ntrue_counts = test_dataset.tf_counts.copy()\nfor n in n_layers_list:\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n  # make predictions\n  pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\n  with torch.no_grad():\n      for batch_idx, data in enumerate(test_dataloader):\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          pred[batch_idx, :, :, :] = profile_pred\n  all_pred = pred.cpu().numpy().copy()\n  assert np.allclose(all_pred.sum(axis=-1), 1)\n          \n  for i, tf in enumerate(conf_dict[\"tf_list\"]): # loop over the four TFs\n      pred_tf = all_pred[:, i, :, :]\n      counts_tf = true_counts[:, i, :, :]\n      labels, predictions, random = binary_labels_from_counts(counts_tf, pred_tf)\n      auprc_score = skm.average_precision_score(labels, predictions)\n      save_scores.append({\"tf\": tf,\n                          \"n_layers\":n,\n                          \"auprc\": auprc_score})\n\n\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n/tmp/ipykernel_2022977/2800911921.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_dil_layers_model.pt\")\n\n\n\n\nCode\ndf = pd.DataFrame(save_scores)\ndf.to_csv(STATS_DIR / \"dil_layers_auprc.csv\")\nsns.scatterplot(data=df, x=\"n_layers\", y=\"auprc\", hue=\"tf\", palette=color_pal)\nplt.show()"
  },
  {
    "objectID": "code/05_architecture_comparison.html#training-only-shape-prediction-1",
    "href": "code/05_architecture_comparison.html#training-only-shape-prediction-1",
    "title": "Architecture Comparisons",
    "section": "5.1 Training (Only Shape Prediction)",
    "text": "5.1 Training (Only Shape Prediction)\n\n\nCode\nn_channel_list = np.array([2, 4, 8, 16, 32, 64, 128, 256])\n\nif retrain_channel:\n  for n_channel in n_channel_list:\n    model = BPNet(n_dil_layers=9, TF_list=conf_dict[\"tf_list\"], pred_total=False, bias_track=True, conv_channels=n_channel).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\n    train_loader=DataLoader(train_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n    tune_loader=DataLoader(tune_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n\n    train_loss, test_loss = [], []\n    patience_counter = 0\n\n    for epoch in range(conf_dict[\"max_epochs\"]):\n      model.train()\n      train_loss_epoch = []\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        optimizer.zero_grad()\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n        train_loss_epoch.append(loss.item())\n        loss.backward()\n        optimizer.step()\n      train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n      # evaluation part\n      test_loss_epoch = []\n      with torch.no_grad():\n          for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n              one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n              profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n              loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n              test_loss_epoch.append(loss.item())\n          test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n      if test_loss[-1] &gt; np.array(test_loss).min():\n        patience_counter += 1\n      else:\n        patience_counter = 0\n        best_state_dict = model.state_dict()\n\n      if patience_counter == conf_dict[\"early_stop_patience\"]:\n        break\n    \n    if conf_dict[\"restore_best_weights\"]:\n      model.load_state_dict(best_state_dict)\n\n    # plot train and test loss\n    plt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\n    plt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # save the model\n    torch.save(obj=model, f=MODELS_DIR / f\"{n_channel}_conv_channel_model.pt\")"
  },
  {
    "objectID": "code/05_architecture_comparison.html#evaluation-1",
    "href": "code/05_architecture_comparison.html#evaluation-1",
    "title": "Architecture Comparisons",
    "section": "5.2 Evaluation",
    "text": "5.2 Evaluation\n\n\nCode\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=PRC_DIR, \n                                  TF_list=conf_dict[\"tf_list\"])\ntest_dataset\n\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n\n\n\n\nCode\nsave_scores = []\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\ntrue_counts = test_dataset.tf_counts.copy()\nfor n in n_channel_list:\n  model = torch.load(MODELS_DIR / f\"{n}_conv_channel_model.pt\")\n  # make predictions\n  pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\n  with torch.no_grad():\n      for batch_idx, data in enumerate(test_dataloader):\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          pred[batch_idx, :, :, :] = profile_pred\n  all_pred = pred.cpu().numpy().copy()\n  assert np.allclose(all_pred.sum(axis=-1), 1)\n          \n  for i, tf in enumerate(conf_dict[\"tf_list\"]): # loop over the four TFs\n      pred_tf = all_pred[:, i, :, :]\n      counts_tf = true_counts[:, i, :, :]\n      labels, predictions, random = binary_labels_from_counts(counts_tf, pred_tf)\n      auprc_score = skm.average_precision_score(labels, predictions)\n      save_scores.append({\"tf\": tf,\n                          \"n_channels\":n,\n                          \"auprc\": auprc_score})\n\n\n/tmp/ipykernel_2022977/949764765.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_conv_channel_model.pt\")\n/tmp/ipykernel_2022977/949764765.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_conv_channel_model.pt\")\n/tmp/ipykernel_2022977/949764765.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_conv_channel_model.pt\")\n/tmp/ipykernel_2022977/949764765.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_conv_channel_model.pt\")\n/tmp/ipykernel_2022977/949764765.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_conv_channel_model.pt\")\n/tmp/ipykernel_2022977/949764765.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_conv_channel_model.pt\")\n/tmp/ipykernel_2022977/949764765.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_conv_channel_model.pt\")\n/tmp/ipykernel_2022977/949764765.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_conv_channel_model.pt\")\n\n\n\n\nCode\ndf = pd.DataFrame(save_scores)\ndf.to_csv(STATS_DIR / \"conv_channel_auprc.csv\")\nsns.scatterplot(data=df, x=\"n_channels\", y=\"auprc\", hue=\"tf\", palette=color_pal)\nplt.show()"
  },
  {
    "objectID": "code/05_architecture_comparison.html#training-only-shape-prediction-2",
    "href": "code/05_architecture_comparison.html#training-only-shape-prediction-2",
    "title": "Architecture Comparisons",
    "section": "6.1 Training (Only Shape Prediction)",
    "text": "6.1 Training (Only Shape Prediction)\n\n\nCode\nkernel_sizes = np.array([5, 9, 13, 17, 21, 25, 29, 33, 37])\n\nif retrain_kern_size:\n  for kern_size in kernel_sizes:\n    model = BPNet(n_dil_layers=9, TF_list=conf_dict[\"tf_list\"], pred_total=False, bias_track=True, conv_channels=64, size_first_kernel=kern_size).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\n    train_loader=DataLoader(train_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n    tune_loader=DataLoader(tune_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n\n    train_loss, test_loss = [], []\n    patience_counter = 0\n\n    for epoch in range(conf_dict[\"max_epochs\"]):\n      model.train()\n      train_loss_epoch = []\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        optimizer.zero_grad()\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n        train_loss_epoch.append(loss.item())\n        loss.backward()\n        optimizer.step()\n      train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n      # evaluation part\n      test_loss_epoch = []\n      with torch.no_grad():\n          for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n              one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n              profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n              loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n              test_loss_epoch.append(loss.item())\n          test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n      if test_loss[-1] &gt; np.array(test_loss).min():\n        patience_counter += 1\n      else:\n        patience_counter = 0\n        best_state_dict = model.state_dict()\n\n      if patience_counter == conf_dict[\"early_stop_patience\"]:\n        break\n    \n    if conf_dict[\"restore_best_weights\"]:\n      model.load_state_dict(best_state_dict)\n\n    # plot train and test loss\n    plt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\n    plt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # save the model\n    torch.save(obj=model, f=MODELS_DIR / f\"{kern_size}_first_kern_size_model.pt\")"
  },
  {
    "objectID": "code/05_architecture_comparison.html#evaluation-2",
    "href": "code/05_architecture_comparison.html#evaluation-2",
    "title": "Architecture Comparisons",
    "section": "6.2 Evaluation",
    "text": "6.2 Evaluation\n\n\nCode\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=PRC_DIR, \n                                  TF_list=conf_dict[\"tf_list\"])\ntest_dataset\n\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n\n\n\n\nCode\nsave_scores = []\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\ntrue_counts = test_dataset.tf_counts.copy()\nfor n in kernel_sizes:\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n  # make predictions\n  pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\n  with torch.no_grad():\n      for batch_idx, data in enumerate(test_dataloader):\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          pred[batch_idx, :, :, :] = profile_pred\n  all_pred = pred.cpu().numpy().copy()\n  assert np.allclose(all_pred.sum(axis=-1), 1)\n          \n  for i, tf in enumerate(conf_dict[\"tf_list\"]): # loop over the four TFs\n      pred_tf = all_pred[:, i, :, :]\n      counts_tf = true_counts[:, i, :, :]\n      labels, predictions, random = binary_labels_from_counts(counts_tf, pred_tf)\n      auprc_score = skm.average_precision_score(labels, predictions)\n      save_scores.append({\"tf\": tf,\n                          \"first_kern_size\": n,\n                          \"auprc\": auprc_score})\n\n\n/tmp/ipykernel_2022977/1834439522.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n/tmp/ipykernel_2022977/1834439522.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n/tmp/ipykernel_2022977/1834439522.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n/tmp/ipykernel_2022977/1834439522.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n/tmp/ipykernel_2022977/1834439522.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n/tmp/ipykernel_2022977/1834439522.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n/tmp/ipykernel_2022977/1834439522.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n/tmp/ipykernel_2022977/1834439522.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n/tmp/ipykernel_2022977/1834439522.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / f\"{n}_first_kern_size_model.pt\")\n\n\n\n\nCode\ndf = pd.DataFrame(save_scores)\ndf.to_csv(STATS_DIR / \"first_kern_size_auprc.csv\")\nsns.scatterplot(data=df, x=\"first_kern_size\", y=\"auprc\", hue=\"tf\", palette=color_pal)\nplt.show()"
  },
  {
    "objectID": "code/03_APIs.html",
    "href": "code/03_APIs.html",
    "title": "02 Dataset and NN Architecture API",
    "section": "",
    "text": "Here we document our dataset API as well as our neural network architecture API"
  },
  {
    "objectID": "code/03_APIs.html#example-1-create-train-dataset-for-all-tfs",
    "href": "code/03_APIs.html#example-1-create-train-dataset-for-all-tfs",
    "title": "02 Dataset and NN Architecture API",
    "section": "3.1 Example 1: Create Train Dataset for all TFs",
    "text": "3.1 Example 1: Create Train Dataset for all TFs\nOne has to provide the set which must be one of train, tune, test as well as the input directory and the list of TFs one wants to model.\n\n\nCode\nwhole_dataset = ChIP_Nexus_Dataset(set_name=\"train\", \n                                   input_dir=PRC_DIR, \n                                   TF_list=['Sox2', 'Oct4', 'Klf4', 'Nanog'])\nwhole_dataset\n\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Sox2', 'Oct4', 'Klf4', 'Nanog']\nSize: 93904\n\n\nCheck the shapes via the check_shapes() method.\n\n\nCode\nwhole_dataset.check_shapes()\n\n\nself.tf_list=['Sox2', 'Oct4', 'Klf4', 'Nanog']\nself.one_hot_seqs.shape=(93904, 4, 1000) [idx, bases, pwidth]\nself.tf_counts.shape=(93904, 4, 2, 1000) [idx, TF, strand, pwidth]\nself.ctrl_counts.shape=(93904, 2, 1000) [idx, strand, pwidth]\nself.ctrl_counts_smooth.shape=(93904, 2, 1000) [idx, strand, pwidth]"
  },
  {
    "objectID": "code/03_APIs.html#example-2-create-train-dataset-for-sox2",
    "href": "code/03_APIs.html#example-2-create-train-dataset-for-sox2",
    "title": "02 Dataset and NN Architecture API",
    "section": "3.2 Example 2: Create Train Dataset for Sox2",
    "text": "3.2 Example 2: Create Train Dataset for Sox2\nIf we only want to take one or a few TFs into consideration we can specify which ones using the TF_list parameter. The constructor method will take care of everything and only keep the peaks that are specific to the TFs in the TF_list.\n\n\nCode\nsmall_dataset = ChIP_Nexus_Dataset(set_name=\"train\", \n                                   input_dir=PRC_DIR, \n                                   TF_list=['Sox2'])\nsmall_dataset\n\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Sox2']\nSize: 6748\n\n\n\n\nCode\nsmall_dataset.check_shapes()\n\n\nself.tf_list=['Sox2']\nself.one_hot_seqs.shape=(6748, 4, 1000) [idx, bases, pwidth]\nself.tf_counts.shape=(6748, 1, 2, 1000) [idx, TF, strand, pwidth]\nself.ctrl_counts.shape=(6748, 2, 1000) [idx, strand, pwidth]\nself.ctrl_counts_smooth.shape=(6748, 2, 1000) [idx, strand, pwidth]"
  },
  {
    "objectID": "code/03_APIs.html#example-3-create-train-dataset-for-sox2-and-high-confidence-peaks",
    "href": "code/03_APIs.html#example-3-create-train-dataset-for-sox2-and-high-confidence-peaks",
    "title": "02 Dataset and NN Architecture API",
    "section": "3.3 Example 3: Create Train Dataset for Sox2 and High-Confidence Peaks",
    "text": "3.3 Example 3: Create Train Dataset for Sox2 and High-Confidence Peaks\nWe might also want to filter peaks based on the qValue.\n\n\nCode\ncutoff = 4.5\nsns.histplot(np.log2(small_dataset.region_info.qValue))\nplt.xlabel(\"Log2 qValue\")\nplt.title(\"Distribution of qValues\")\nplt.axvline(cutoff, color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLooking at the histogram of the log2 qValue, we might decide to only keep peaks with a log2 qValue above 4.5.\n\n\nCode\nhighconf_dataset = ChIP_Nexus_Dataset(set_name=\"train\", \n                                      input_dir=PRC_DIR, \n                                      TF_list=[\"Sox2\"],\n                                      qval_thr=2**cutoff)\nhighconf_dataset\n\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Sox2']\nSize: 4182\n\n\n\n\nCode\nhighconf_dataset.check_shapes()\n\n\nself.tf_list=['Sox2']\nself.one_hot_seqs.shape=(4182, 4, 1000) [idx, bases, pwidth]\nself.tf_counts.shape=(4182, 1, 2, 1000) [idx, TF, strand, pwidth]\nself.ctrl_counts.shape=(4182, 2, 1000) [idx, strand, pwidth]\nself.ctrl_counts_smooth.shape=(4182, 2, 1000) [idx, strand, pwidth]"
  },
  {
    "objectID": "code/03_APIs.html#example-4-create-train-dataset-for-sox2-but-keep-all-regions",
    "href": "code/03_APIs.html#example-4-create-train-dataset-for-sox2-but-keep-all-regions",
    "title": "02 Dataset and NN Architecture API",
    "section": "3.4 Example 4: Create Train Dataset for Sox2 but keep all Regions",
    "text": "3.4 Example 4: Create Train Dataset for Sox2 but keep all Regions\nNow we might also want to create a training set that contains all the regions but only the counts for Sox2.\n\n\nCode\nsox2_all_regions = ChIP_Nexus_Dataset(set_name=\"train\", \n                                      input_dir=PRC_DIR, \n                                      TF_list=[\"Sox2\"], \n                                      subset=False)\nsox2_all_regions\n\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Sox2']\nSize: 93904\n\n\n\n\nCode\nsox2_all_regions.check_shapes()\n\n\nself.tf_list=['Sox2']\nself.one_hot_seqs.shape=(93904, 4, 1000) [idx, bases, pwidth]\nself.tf_counts.shape=(93904, 1, 2, 1000) [idx, TF, strand, pwidth]\nself.ctrl_counts.shape=(93904, 2, 1000) [idx, strand, pwidth]\nself.ctrl_counts_smooth.shape=(93904, 2, 1000) [idx, strand, pwidth]"
  },
  {
    "objectID": "code/03_APIs.html#example-1-one-tf-shape-prediction-no-bias-track",
    "href": "code/03_APIs.html#example-1-one-tf-shape-prediction-no-bias-track",
    "title": "02 Dataset and NN Architecture API",
    "section": "4.1 Example 1: One TF, Shape Prediction, No Bias Track",
    "text": "4.1 Example 1: One TF, Shape Prediction, No Bias Track\n\n\nCode\nmodel_1 = BPNet(n_dil_layers=9, TF_list=[\"Sox2\"], \n                pred_total=False, bias_track=False)\nmodel_1\n\n\nBPNet(\n  (base_model): ConvLayers(\n    (conv_layers): ModuleList(\n      (0): Conv1d(4, 64, kernel_size=(25,), stride=(1,), padding=same)\n      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n      (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(16,))\n      (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(32,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(64,))\n      (7): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(128,))\n      (8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(256,))\n      (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(512,))\n    )\n  )\n  (profile_heads): ModuleList(\n    (0): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n  )\n)"
  },
  {
    "objectID": "code/03_APIs.html#example-2-one-tf-shape-total-counts-prediction-no-bias-track",
    "href": "code/03_APIs.html#example-2-one-tf-shape-total-counts-prediction-no-bias-track",
    "title": "02 Dataset and NN Architecture API",
    "section": "4.2 Example 2: One TF, Shape & Total Counts Prediction, No Bias Track",
    "text": "4.2 Example 2: One TF, Shape & Total Counts Prediction, No Bias Track\n\n\nCode\nmodel_2 = BPNet(n_dil_layers=9, TF_list=[\"Sox2\"], pred_total=True, bias_track=False)\nmodel_2\n\n\nBPNet(\n  (base_model): ConvLayers(\n    (conv_layers): ModuleList(\n      (0): Conv1d(4, 64, kernel_size=(25,), stride=(1,), padding=same)\n      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n      (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(16,))\n      (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(32,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(64,))\n      (7): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(128,))\n      (8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(256,))\n      (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(512,))\n    )\n  )\n  (profile_heads): ModuleList(\n    (0): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n  )\n  (count_heads): ModuleList(\n    (0): TotalCountHead(\n      (fc1): Linear(in_features=64, out_features=32, bias=True)\n      (fc2): Linear(in_features=32, out_features=2, bias=True)\n    )\n  )\n)"
  },
  {
    "objectID": "code/03_APIs.html#example-3-one-tf-shape-total-counts-prediction-bias",
    "href": "code/03_APIs.html#example-3-one-tf-shape-total-counts-prediction-bias",
    "title": "02 Dataset and NN Architecture API",
    "section": "4.3 Example 3: One TF, Shape & Total Counts Prediction, Bias",
    "text": "4.3 Example 3: One TF, Shape & Total Counts Prediction, Bias\n\n\nCode\nmodel_3 = BPNet(n_dil_layers=9, TF_list=[\"Sox2\"], pred_total=True, bias_track=True)\nmodel_3\n\n\nBPNet(\n  (base_model): ConvLayers(\n    (conv_layers): ModuleList(\n      (0): Conv1d(4, 64, kernel_size=(25,), stride=(1,), padding=same)\n      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n      (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(16,))\n      (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(32,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(64,))\n      (7): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(128,))\n      (8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(256,))\n      (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(512,))\n    )\n  )\n  (profile_heads): ModuleList(\n    (0): ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n  )\n  (count_heads): ModuleList(\n    (0): TotalCountHead(\n      (fc1): Linear(in_features=64, out_features=32, bias=True)\n      (fc2): Linear(in_features=32, out_features=2, bias=True)\n    )\n  )\n)\n\n\nFeatures bias weights.\n\n\nCode\nmodel_3.profile_heads[0].bias_weights\n\n\nParameter containing:\ntensor([0.0100, 0.0100], requires_grad=True)"
  },
  {
    "objectID": "code/03_APIs.html#example-4-all-tfs-shape-total-counts-prediction-bias",
    "href": "code/03_APIs.html#example-4-all-tfs-shape-total-counts-prediction-bias",
    "title": "02 Dataset and NN Architecture API",
    "section": "4.4 Example 4: All TFs, Shape & Total Counts Prediction, Bias",
    "text": "4.4 Example 4: All TFs, Shape & Total Counts Prediction, Bias\n\n\nCode\nmodel_4 = BPNet(n_dil_layers=9, TF_list=[\"Sox2\", \"Oct4\", \"Nanog\", \"Klf4\"], pred_total=True, bias_track=True)\nmodel_4\n\n\nBPNet(\n  (base_model): ConvLayers(\n    (conv_layers): ModuleList(\n      (0): Conv1d(4, 64, kernel_size=(25,), stride=(1,), padding=same)\n      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n      (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(16,))\n      (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(32,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(64,))\n      (7): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(128,))\n      (8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(256,))\n      (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same, dilation=(512,))\n    )\n  )\n  (profile_heads): ModuleList(\n    (0-3): 4 x ProfileShapeHead(\n      (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n    )\n  )\n  (count_heads): ModuleList(\n    (0-3): 4 x TotalCountHead(\n      (fc1): Linear(in_features=64, out_features=32, bias=True)\n      (fc2): Linear(in_features=32, out_features=2, bias=True)\n    )\n  )\n)"
  },
  {
    "objectID": "code/03_APIs.html#recreate-figure-1-e",
    "href": "code/03_APIs.html#recreate-figure-1-e",
    "title": "02 Dataset and NN Architecture API",
    "section": "5.1 Recreate Figure 1 e",
    "text": "5.1 Recreate Figure 1 e\n\n\nCode\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=PRC_DIR, \n                                  TF_list=['Oct4', 'Sox2', 'Nanog', 'Klf4'])\ntest_dataset\n\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Oct4', 'Sox2', 'Nanog', 'Klf4']\nSize: 27727\n\n\n\n\nCode\ntmp_df = test_dataset.region_info.copy().reset_index()\nidx = tmp_df.loc[(tmp_df.seqnames==\"chr1\") & (tmp_df.start &gt; 180924752-1000) & (tmp_df.end &lt; 180925152+1000)].index.to_numpy()[0]\n\ndiff = 180924752 - tmp_df.start[idx] + 1\nw = 400\n\nfig, axis = plt.subplots(4, 1, figsize=(6, 14))\n\nfor ax, (i, tf) in zip(axis, enumerate(test_dataset.tf_list)):\n  ax.plot(test_dataset.tf_counts[idx, i, 0, diff:(diff+w)], label=\"pos\")\n  ax.plot(-test_dataset.tf_counts[idx, i, 1, diff:(diff+w)], label=\"neg\")\n  ax.legend()\n  ax.set_title(tf)\nplt.show()"
  },
  {
    "objectID": "code/03_APIs.html#check-one-hot-encoding",
    "href": "code/03_APIs.html#check-one-hot-encoding",
    "title": "02 Dataset and NN Architecture API",
    "section": "5.2 Check One-Hot Encoding",
    "text": "5.2 Check One-Hot Encoding\nTo check whether the one-hot encoding worked as expected, we compare here:\n\nThe one-hot encoded sequence as stored in the test dataset\n\n\n\nCode\nplt.imshow(test_dataset.one_hot_seqs[idx, :, diff:(diff+w)], interpolation=\"none\", aspect=\"auto\")\nplt.title(\"One-Hot Encoding from Test Dataset\")\nplt.yticks([0, 1, 2, 3], labels=[\"A\", \"C\", \"G\", \"T\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe one-hot encoded sequence obtained from reading in the mm10 genome and one-hot encoding corresponding sequence\n\n\n\nCode\n#from Bio.Seq import Seq\n#from Bio import SeqIO\n#mm10_ref = SeqIO.to_dict(SeqIO.parse(DATA_DIR / \"mm10.fa\", \"fasta\"))\n#seq = mm10_ref[tmp_df.iloc[idx][\"seqnames\"]][180924752:180925152]\n#one_hot_seq = np.zeros((4, 400))\n#for i, letter in enumerate(np.array(seq.seq)):\n#  if letter==\"A\": one_hot_seq[0, i] = 1\n#  if letter==\"C\": one_hot_seq[1, i] = 1\n#  if letter==\"G\": one_hot_seq[2, i] = 1\n#  if letter==\"T\": one_hot_seq[3, i] = 1\n#plt.imshow(one_hot_seq, interpolation=\"none\", aspect=\"auto\")\n#plt.yticks([0, 1, 2, 3], labels=[\"A\", \"C\", \"G\", \"T\"])\n#plt.title(\"One-Hot Encoding based on Reference Sequence\")\n#plt.show()\n\n\n\n\nCode\nimport gzip\nfrom Bio.Seq import Seq\nfrom Bio import SeqIO\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata_dir = Path(\"..\") / \"data\"\nmm10_path = data_dir / \"mm10.fa.gz\"\n\n# Parse the gzipped file on-the-fly\nwith gzip.open(mm10_path, \"rt\") as handle:\n    mm10_ref = SeqIO.to_dict(SeqIO.parse(handle, \"fasta\"))\nseq = mm10_ref[tmp_df[\"seqnames\"][idx]].seq[180924752:180925152]\n\none_hot_seq = np.zeros((4, len(seq)))\nfor i, letter in enumerate(seq):\n    if letter == \"A\": one_hot_seq[0, i] = 1\n    elif letter == \"C\": one_hot_seq[1, i] = 1\n    elif letter == \"G\": one_hot_seq[2, i] = 1\n    elif letter == \"T\": one_hot_seq[3, i] = 1\n\nplt.imshow(one_hot_seq, interpolation=\"none\", aspect=\"auto\")\nplt.yticks([0, 1, 2, 3], labels=[\"A\", \"C\", \"G\", \"T\"])\nplt.title(\"One-Hot Encoding based on Reference Sequence\")\nplt.xlabel(\"Position\")\nplt.ylabel(\"Base\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfor the peak seen in Figure 1e\n\n\nCode\nnp.all(test_dataset.one_hot_seqs[idx, :, diff:(diff+w)] == one_hot_seq)\n\n\nnp.True_\n\n\nAnd we see that we get exactly the same."
  },
  {
    "objectID": "code/01_download.html",
    "href": "code/01_download.html",
    "title": "Data Download",
    "section": "",
    "text": "We can downlaod the data from https://zenodo.org/records/3371216\n\n\n\nCode\nimport requests\nfrom pathlib import Path\nimport tarfile\n\ndata_tar = Path(\"..\") / \"data.tar.gz\"\ndata_directory = Path(\"..\") / \"data\"\n\nif data_directory.exists():\n    print(f\"Data directory already exists at {data_directory}.\")\nelif data_tar.exists():\n    print(f\"Tar.gz file found at {data_tar}. Extracting...\")\n    with tarfile.open(data_tar, \"r:gz\") as tar_ref:\n        tar_ref.extractall(data_directory)\n    print(\"Extraction complete.\")\n    # Delete the tar file\n    data_tar.unlink()\n    print(f\"Deleted tar.gz file at {data_tar}.\")\nelse:\n    data_url = \"https://zenodo.org/records/3371216/files/data.tar.gz?download=1\"\n    response = requests.get(data_url, stream=True)\n    response.raise_for_status()  # Raise an error for bad HT\n    with open(data_tar, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    print(f\"Downloaded and saved tar.gz file at {data_tar}.\")\n    print(\"Extracting the tar.gz file...\")\n    with tarfile.open(data_tar, \"r:gz\") as tar_ref:\n        tar_ref.extractall(data_directory)\n    print(\"Extraction complete.\")\n    data_tar.unlink()\n    print(f\"Deleted tar.gz file at {data_tar}.\")\nlist(data_directory.iterdir())\n\n\nData directory already exists at ../data.\n\n\n[PosixPath('../data/mm10.fa.gz'), PosixPath('../data/data')]\n\n\n\nSince the data directory is quite large we will delete the files we dont need.\nFurthermore we download the mm10.fa file\n\n\n\nCode\nmm10_file = data_directory / \"mm10.fa.gz\"\nmm10_url = \"https://hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/mm10.fa.gz\"\n# Check if the file already exists\nif mm10_file.exists():\n    print(f\"File already exists at {mm10_file}. Skipping download.\")\nelse:\n    # Download the file\n    print(f\"Downloading {mm10_url}...\")\n    response = requests.get(mm10_url, stream=True)\n    response.raise_for_status()  # Raise an error for bad HTTP responses\n\n    # Save the file\n    with open(mm10_file, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    print(f\"Downloaded and saved file at {mm10_file}.\")\n\n\nFile already exists at ../data/mm10.fa.gz. Skipping download."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "1 TOC\n\nIntroduction\nData Download\nData Preprocessing\nAPI"
  },
  {
    "objectID": "code/00_introduction.html",
    "href": "code/00_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "1 TODO\nWhat data do we have, what are we predicting and why are we doing that?\n\n\n2 What\n\n\nNotes:\n\nActually they used an adaption of ChIP-seq called ChIP-seq Nexus which comprises an additional exonuclease step yielding an increased resolution."
  },
  {
    "objectID": "code/02_prepare_input.html",
    "href": "code/02_prepare_input.html",
    "title": "Prepare Input",
    "section": "",
    "text": "Here we generate the model input based on the raw ChIP-Nexus data."
  },
  {
    "objectID": "code/02_prepare_input.html#data-download",
    "href": "code/02_prepare_input.html#data-download",
    "title": "Prepare Input",
    "section": "3.1 Data Download",
    "text": "3.1 Data Download\n\nThe data can be downloaded from: https://zenodo.org/records/3371216 (~ 30 GBs) as shown in the download.qmd file.\n\n\n\nCode\ndata_dir &lt;- file.path(\"..\", \"data\")"
  },
  {
    "objectID": "code/02_prepare_input.html#output-directory",
    "href": "code/02_prepare_input.html#output-directory",
    "title": "Prepare Input",
    "section": "3.2 Output Directory",
    "text": "3.2 Output Directory\n\n\nCode\noutput_dir &lt;- file.path(\"..\", \"prc\")\nfigure_dir &lt;- file.path(\"..\", \"figures\", \"prepare_input\")\ndir.create(output_dir, recursive=TRUE, showWarnings=FALSE)\ndir.create(figure_dir, recursive=TRUE, showWarnings=FALSE)\n\nTFs &lt;- c(\"Sox2\", \"Oct4\", \"Klf4\", \"Nanog\")\n\npurrr::walk(c(TFs, \"patchcap\"), function(dir_name) {\n  dir.create(file.path(output_dir, dir_name), recursive=TRUE, showWarnings=FALSE)\n})"
  },
  {
    "objectID": "code/02_prepare_input.html#peak-width",
    "href": "code/02_prepare_input.html#peak-width",
    "title": "Prepare Input",
    "section": "3.3 Peak Width",
    "text": "3.3 Peak Width\nAs in the BPNet paper, we are using a peak width of 1000 bp, meaning we consider 500 bp up- and downstream of the ChIP-seq peaks.\n\n\nCode\npeak_width &lt;- 1000"
  },
  {
    "objectID": "code/02_prepare_input.html#chromosome-traintunetest-split",
    "href": "code/02_prepare_input.html#chromosome-traintunetest-split",
    "title": "Prepare Input",
    "section": "3.4 Chromosome Train/Tune/Test split",
    "text": "3.4 Chromosome Train/Tune/Test split\n\n\nCode\nall_chroms &lt;- paste0(\"chr\", c(1:19, \"X\", \"Y\"))\n\n# we use the same train/tune/test split as in the BPnet paper\nchrom_list &lt;- list(\"tune\" = c(\"chr2\", \"chr3\", \"chr4\"), # tune set (hyperparameter tuning): chromosomes 2, 3, 4\n                   \"test\" = c(\"chr1\", \"chr8\", \"chr9\")) # test set (performance evaluation): chromosome 1, 8, 9\nchrom_list$train &lt;- setdiff(all_chroms, c(chrom_list$tune, chrom_list$test)) # train set: all other chromosomes\nchrom_list\n\n\n$tune\n[1] \"chr2\" \"chr3\" \"chr4\"\n\n$test\n[1] \"chr1\" \"chr8\" \"chr9\"\n\n$train\n [1] \"chr5\"  \"chr6\"  \"chr7\"  \"chr10\" \"chr11\" \"chr12\" \"chr13\" \"chr14\" \"chr15\"\n[10] \"chr16\" \"chr17\" \"chr18\" \"chr19\" \"chrX\"  \"chrY\""
  },
  {
    "objectID": "code/02_prepare_input.html#colors",
    "href": "code/02_prepare_input.html#colors",
    "title": "Prepare Input",
    "section": "3.5 Colors",
    "text": "3.5 Colors\n\n\nCode\ncolors = c(\"Klf4\" = \"#92C592\",\n           \"Nanog\" = \"#FFE03F\",\n           \"Oct4\" = \"#CD5C5C\",\n           \"Sox2\" = \"#849EEB\",\n           \"patchcap\" = \"#827F81\")"
  },
  {
    "objectID": "code/02_prepare_input.html#distribution-of-the-macs-scores",
    "href": "code/02_prepare_input.html#distribution-of-the-macs-scores",
    "title": "Prepare Input",
    "section": "6.1 Distribution of the MACS Scores",
    "text": "6.1 Distribution of the MACS Scores\n\nWe see that the distribution of MACS scores (qValues) is quite comparable for the different TFs.\n\n\n\nCode\np &lt;- peak_infos %&gt;%\n  as.data.frame() %&gt;%\n  ggplot() +\n  geom_density(aes(x=log2(qValue), color=TF), alpha=0.4, fill=NA) +\n  labs(x=\"Log2 qValue\", y=\"Density\", color=\"TF\") +\n  scale_color_manual(values=colors) +\n  theme_bw()\nggsave(filename = file.path(figure_dir, \"macs2_scores_per_tf.pdf\"),\n       plot = p, width = 4, height = 4)\np +\n  ggdark::dark_mode(verbose=FALSE)"
  },
  {
    "objectID": "code/02_prepare_input.html#number-of-peaks-per-tf",
    "href": "code/02_prepare_input.html#number-of-peaks-per-tf",
    "title": "Prepare Input",
    "section": "6.2 Number of Peaks per TF",
    "text": "6.2 Number of Peaks per TF\n\nHowever the number of peaks is quite different. There are many more peaks for Nanog and Klf4.\nWhat does this mean for the training?\n\n\n\nCode\np &lt;- peak_infos %&gt;%\n  as.data.frame() %&gt;%\n  ggplot() +\n  geom_bar(aes(y=TF, fill=TF), alpha=0.4, width=0.4, color=\"black\") +\n  labs(x=\"Number of Peaks\", y=\"TF\") +\n  theme_bw() +\n  scale_fill_manual(values=colors)\nggsave(filename = file.path(figure_dir, \"n_peaks_per_tf.pdf\"),\n       plot = p, width = 4, height = 4)\np +\n  ggdark::dark_mode(verbose=FALSE)"
  },
  {
    "objectID": "code/02_prepare_input.html#examples-highest-scoring-peaks",
    "href": "code/02_prepare_input.html#examples-highest-scoring-peaks",
    "title": "Prepare Input",
    "section": "8.1 Examples: Highest Scoring Peaks",
    "text": "8.1 Examples: Highest Scoring Peaks\n\n\nClick to expand\n\n\n\nCode\ntest_df &lt;- peak_infos %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::filter(set==\"train\") %&gt;%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %&gt;%\n  dplyr::group_by(TF) %&gt;%\n  slice_max(order_by=qValue, n=2) %&gt;%\n  select(Region, TF, qValue)\n\npurrr::walk(1:nrow(test_df), function(i) {\n  test_instance &lt;- unlist(test_df[i, ])\n  purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                 minus = -tf_counts[[tf]]$train$neg[match(test_instance[\"Region\"], seq_names$train), ]) %&gt;%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %&gt;% \n  pivot_longer(cols=c(minus, plus)) %&gt;%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=name)) +\n  facet_wrap(~TF, scales=\"free_y\") +\n  ggdark::dark_mode(verbose=FALSE) +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=paste0(\"Peak \", test_instance[\"Region\"], \" | \", test_instance[\"TF\"], \n                    \" | \", test_instance[\"qValue\"])) -&gt; p\n  print(p)\n})"
  },
  {
    "objectID": "code/02_prepare_input.html#examples-low-scoring-peaks",
    "href": "code/02_prepare_input.html#examples-low-scoring-peaks",
    "title": "Prepare Input",
    "section": "8.2 Examples: Low Scoring Peaks",
    "text": "8.2 Examples: Low Scoring Peaks\n\n\nClick to expand\n\n\n\nCode\nset.seed(42)\ntest_df &lt;- peak_infos %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::filter(set==\"train\") %&gt;%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %&gt;%\n  dplyr::group_by(TF) %&gt;%\n  slice_min(order_by=qValue, n=2) %&gt;%\n  select(Region, TF, qValue)\n\npurrr::walk(1:nrow(test_df), function(i) {\n  test_instance &lt;- unlist(test_df[i, ])\n  purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                 minus = -tf_counts[[tf]]$train$neg[match(test_instance[\"Region\"], seq_names$train), ]) %&gt;%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %&gt;% \n  pivot_longer(cols=c(minus, plus)) %&gt;%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=name)) +\n  facet_wrap(~TF, scales=\"free_y\") +\n  ggdark::dark_mode(verbose=FALSE) +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=paste0(\"Peak \", test_instance[\"Region\"], \" | \", test_instance[\"TF\"], \n                    \" | \", test_instance[\"qValue\"])) -&gt; p\n  print(p)\n})"
  },
  {
    "objectID": "code/02_prepare_input.html#examples-highest-scoring-peaks-with-bias",
    "href": "code/02_prepare_input.html#examples-highest-scoring-peaks-with-bias",
    "title": "Prepare Input",
    "section": "9.1 Examples: Highest Scoring Peaks with Bias",
    "text": "9.1 Examples: Highest Scoring Peaks with Bias\n\n\nCode\ntest_instance &lt;- peak_infos %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::filter(set==\"test\") %&gt;%\n  dplyr::filter(seqnames==\"chr1\", start &gt;= 180924752-1000, end &lt;= 180925152+1000) %&gt;%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %&gt;%\n  dplyr::select(Region, TF, qValue) %&gt;%\n  .[1, ]\nprint(test_instance)\n\n\n                    Region   TF   qValue\n1 chr1:180924435-180925434 Sox2 436.0653\n\n\nCode\np &lt;- purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$test$pos[match(test_instance[\"Region\"], seq_names$test), ],\n                 minus = -tf_counts[[tf]]$test$neg[match(test_instance[\"Region\"], seq_names$test), ]) %&gt;%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %&gt;% \n  rbind(\n    tibble::tibble(position=-499:500, \n                   plus = ctrl_counts$test$pos[match(test_instance[\"Region\"], seq_names$test), ],\n                   minus = -ctrl_counts$test$neg[match(test_instance[\"Region\"], seq_names$test), ],\n                   TF = \"patchcap\", p_name = test_instance[\"Region\"])\n  ) %&gt;%\n  pivot_longer(cols=c(minus, plus)) %&gt;%\n  dplyr::mutate(TF = factor(TF, levels=names(colors))) %&gt;%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=TF, alpha=name), size=0.2) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=test_instance[\"Region\"]) +\n  #scale_color_manual(values=c(\"minus\"=\"darkred\", \"plus\"=\"forestgreen\")) +\n  scale_color_manual(values=colors) +\n  scale_alpha_manual(values=c(\"plus\" = 1, \"minus\" = 1)) +\n  theme_bw() +\n  theme(plot.title = element_text(size=10, hjust=0.5), strip.background = element_rect(fill=NA))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n\n\nCode\nggsave(filename = file.path(figure_dir, \"example_high_q.pdf\"), \n       plot = p, width = 4, height = 8)\np + \n  ggdark::dark_mode(verbose=FALSE)\n\n\n\n\n\n\n\n\n\n\n\nClick to view more plots\n\n\n\nCode\ntest_df &lt;- peak_infos %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::filter(set==\"train\") %&gt;%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %&gt;%\n  dplyr::group_by(TF) %&gt;%\n  slice_max(order_by=qValue, n=5) %&gt;%\n  select(Region, TF, qValue)\npurrr::walk(1:nrow(test_df), function(i) {\n  test_instance &lt;- unlist(test_df[i, ])\n  purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                 minus = -tf_counts[[tf]]$train$neg[match(test_instance[\"Region\"], seq_names$train), ]) %&gt;%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %&gt;% \n  rbind(\n    tibble::tibble(position=-499:500, \n                   plus = ctrl_counts$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                   minus = -ctrl_counts$train$neg[match(test_instance[\"Region\"], seq_names$train), ],\n                   TF = \"Bias\", p_name = test_instance[\"Region\"])\n  ) %&gt;%\n  pivot_longer(cols=c(minus, plus)) %&gt;%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=name)) +\n  facet_wrap(~TF, scales=\"free_y\") +\n  ggdark::dark_mode(verbose=FALSE) +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=paste0(\"Peak \", test_instance[\"Region\"], \" | \", test_instance[\"TF\"], \n                    \" | \", test_instance[\"qValue\"])) -&gt; p\n  print(p)\n})"
  },
  {
    "objectID": "code/02_prepare_input.html#examples-random-peaks-with-bias",
    "href": "code/02_prepare_input.html#examples-random-peaks-with-bias",
    "title": "Prepare Input",
    "section": "9.2 Examples: Random Peaks with Bias",
    "text": "9.2 Examples: Random Peaks with Bias\n\n\nCode\ntest_instance &lt;- peak_infos %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::filter(set==\"test\") %&gt;%\n  dplyr::filter(seqnames==\"chr1\", start &gt;= 4000, end &lt;= 100000000) %&gt;%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %&gt;%\n  dplyr::select(Region, TF, qValue) %&gt;%\n  .[1000, ]\nprint(test_instance)\n\n\n                     Region   TF   qValue\n1000 chr1:58392603-58393602 Oct4 21.10429\n\n\nCode\np &lt;- purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$test$pos[match(test_instance[\"Region\"], seq_names$test), ],\n                 minus = -tf_counts[[tf]]$test$neg[match(test_instance[\"Region\"], seq_names$test), ]) %&gt;%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %&gt;% \n  rbind(\n    tibble::tibble(position=-499:500, \n                   plus = ctrl_counts$test$pos[match(test_instance[\"Region\"], seq_names$test), ],\n                   minus = -ctrl_counts$test$neg[match(test_instance[\"Region\"], seq_names$test), ],\n                   TF = \"patchcap\", p_name = test_instance[\"Region\"])\n  ) %&gt;%\n  pivot_longer(cols=c(minus, plus)) %&gt;%\n  dplyr::mutate(TF = factor(TF, levels=names(colors))) %&gt;%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=TF, alpha=name), size=0.2) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=test_instance[\"Region\"]) +\n  #scale_color_manual(values=c(\"minus\"=\"darkred\", \"plus\"=\"forestgreen\")) +\n  scale_color_manual(values=colors) +\n  scale_alpha_manual(values=c(\"plus\" = 1, \"minus\" = 1)) +\n  theme_bw() +\n  theme(plot.title = element_text(size=10, hjust=0.5), strip.background = element_rect(fill=NA))\nggsave(filename = file.path(figure_dir, \"example_low_q.pdf\"), \n       plot = p, width = 4, height = 8)\np + \n  ggdark::dark_mode(verbose=FALSE)\n\n\n\n\n\n\n\n\n\n\n\nClick to view more plots\n\n\n\nCode\nset.seed(42)\ntest_df &lt;- peak_infos %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::filter(set==\"train\") %&gt;%\n  dplyr::mutate(Region = paste0(seqnames, \":\", start, \"-\", end)) %&gt;%\n  dplyr::group_by(TF) %&gt;%\n  slice_sample(n=5) %&gt;%\n  select(Region, TF, qValue)\n\npurrr::walk(1:nrow(test_df), function(i) {\n  test_instance &lt;- unlist(test_df[i, ])\n  purrr::map_dfr(TFs, function(tf){\n  tibble::tibble(position=-499:500, \n                 plus = tf_counts[[tf]]$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                 minus = -tf_counts[[tf]]$train$neg[match(test_instance[\"Region\"], seq_names$train), ]) %&gt;%\n    dplyr::mutate(TF = tf, p_name = test_instance[\"Region\"])\n  }) %&gt;% \n  rbind(\n    tibble::tibble(position=-499:500, \n                   plus = ctrl_counts$train$pos[match(test_instance[\"Region\"], seq_names$train), ],\n                   minus = -ctrl_counts$train$neg[match(test_instance[\"Region\"], seq_names$train), ],\n                   TF = \"Bias\", p_name = test_instance[\"Region\"])\n  ) %&gt;%\n  pivot_longer(cols=c(minus, plus)) %&gt;%\n  ggplot() +\n  geom_line(aes(x=position, y=value, color=name)) +\n  facet_wrap(~TF, scales=\"free_y\") +\n  ggdark::dark_mode(verbose=FALSE) +\n  labs(x=\"Relative Position [bp]\", y=\"Counts\", color=\"Strand\", \n       title=paste0(\"Peak \", test_instance[\"Region\"], \" | \", test_instance[\"TF\"], \n                    \" | \", test_instance[\"qValue\"])) -&gt; p\n  print(p)\n})"
  },
  {
    "objectID": "code/02_prepare_input.html#from-raw-data",
    "href": "code/02_prepare_input.html#from-raw-data",
    "title": "Prepare Input",
    "section": "10.1 From Raw Data",
    "text": "10.1 From Raw Data\n\n\nCode\nroi &lt;- list(\"seqname\"=\"chr1\", \"start\"=180924752, \"end\"=180925152)\nTFs &lt;- c(\"Oct4\", \"Sox2\", \"Nanog\", \"Klf4\")\ndf &lt;-\n  purrr::map_dfr(TFs, function(tf) {\n    alignments &lt;- readGAlignments(file.path(data_dir, \"chip-nexus\", tf, \"pool_filt.bam\"),\n                                  param = ScanBamParam(which=GRanges(paste0(roi$seqname, \":\", roi$start, \"-\", roi$end))))\n    align_pos &lt;- alignments[strand(alignments)==\"+\"]\n    align_neg &lt;- alignments[strand(alignments)==\"-\"]\n\n    align_pos@cigar &lt;- rep(\"1M\", length(align_pos))\n    align_neg@start &lt;- GenomicAlignments::end(align_neg)\n    align_neg@cigar &lt;- rep(\"1M\", length(align_neg))\n\n    tibble::tibble(pos=as.numeric(coverage(align_pos)$chr1[roi$start:roi$end]),\n                   neg=-as.numeric(coverage(align_neg)$chr1[roi$start:roi$end]),\n                   position=roi$start:roi$end,\n                   TF=tf)\n  }) %&gt;%\n  pivot_longer(cols=c(\"pos\", \"neg\"))\ndf\n\n\n\n  \n\n\n\n\n\nCode\ndf %&gt;%\n  mutate(TF = factor(TF, levels=c(TFs))) %&gt;%\n  ggplot() +\n  geom_line(aes(x=position, y=value, col=name)) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  ggdark::dark_mode(verbose=FALSE)"
  },
  {
    "objectID": "code/02_prepare_input.html#from-processed-data",
    "href": "code/02_prepare_input.html#from-processed-data",
    "title": "Prepare Input",
    "section": "10.2 From Processed Data",
    "text": "10.2 From Processed Data\n\n\nCode\nroi &lt;- list(\"seqname\"=\"chr1\", \"start\"=180924752, \"end\"=180925152)\nroi_adjusted &lt;- roi\nroi_adjusted$start = roi_adjusted$start-1000\nroi_adjusted$end = roi_adjusted$end+1000\n\nTFs &lt;- c(\"Oct4\", \"Sox2\", \"Nanog\", \"Klf4\")\ndf2 &lt;- \n  purrr::map_dfr(TFs, function(tf) {\n    cov_list &lt;- list(\"pos\" = tf_counts[[tf]]$test$pos,\n                     \"neg\" = tf_counts[[tf]]$test$neg)\n    rnames &lt;- seq_names$test\n    gr_rnames &lt;- GRanges(rnames)\n    \n    bool_vec &lt;- \n      (as.character(gr_rnames@seqnames) == roi_adjusted$seqname &\n      start(gr_rnames@ranges) &gt;= roi_adjusted$start &\n      end(gr_rnames@ranges) &lt;= roi_adjusted$end)\n    \n    peak_index &lt;- which(bool_vec)[1]\n    peak_info &lt;- gr_rnames[peak_index]\n    \n    diff &lt;- 180924752 - start(peak_info@ranges) + 1\n    w &lt;- 400\n    \n    tibble::tibble(pos=cov_list$pos[peak_index, diff:(diff+w)],\n                   neg=-cov_list$neg[peak_index, diff:(diff+w)],\n                   position=0:400,\n                   TF=tf)\n}) %&gt;%\n  pivot_longer(cols=c(\"pos\", \"neg\"))\ndf2\n\n\n\n  \n\n\n\n\n\nCode\ndf2 %&gt;%\n  mutate(TF = factor(TF, levels=c(TFs))) %&gt;%\n  ggplot() +\n  geom_line(aes(x=position, y=value, col=name)) +\n  facet_wrap(~TF, ncol=1, scales=\"free_y\") +\n  labs(x=\"Position\", y=\"Counts\", col=\"Strand\") +\n  ggdark::dark_mode(verbose=FALSE)\n\n\n\n\n\n\n\n\n\nCompare.\n\n\nCode\nall(df2$value == df$value)\n\n\n[1] TRUE\n\n\nCode\nall(df2$TF == df$TF)\n\n\n[1] TRUE\n\n\nCode\nall(df2$name == df$name)\n\n\n[1] TRUE"
  },
  {
    "objectID": "code/02_prepare_input.html#save-image",
    "href": "code/02_prepare_input.html#save-image",
    "title": "Prepare Input",
    "section": "12.1 Save Image",
    "text": "12.1 Save Image\n\n\nCode\nsave.image(file.path(output_dir, \"tmp.RData\"))"
  },
  {
    "objectID": "code/02_prepare_input.html#session-info",
    "href": "code/02_prepare_input.html#session-info",
    "title": "Prepare Input",
    "section": "12.2 Session Info",
    "text": "12.2 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-conda-linux-gnu (64-bit)\nRunning under: Ubuntu 24.04 LTS\n\nMatrix products: default\nBLAS/LAPACK: /home/pschaefer/miniconda3/envs/r-env-bpnet/lib/libopenblasp-r0.3.28.so;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Berlin\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] plyranges_1.22.0                   TFBSTools_1.40.0                  \n [3] JASPAR2020_0.99.10                 motifmatchr_1.24.0                \n [5] furrr_0.3.1                        future_1.34.0                     \n [7] lubridate_1.9.3                    forcats_1.0.0                     \n [9] stringr_1.5.1                      dplyr_1.1.4                       \n[11] purrr_1.0.2                        readr_2.1.5                       \n[13] tidyr_1.3.1                        tibble_3.2.1                      \n[15] ggplot2_3.5.1                      tidyverse_2.0.0                   \n[17] BSgenome.Mmusculus.UCSC.mm10_1.4.3 BSgenome_1.70.1                   \n[19] BiocIO_1.12.0                      BRGenomics_1.13.0                 \n[21] GenomicAlignments_1.38.0           Rsamtools_2.18.0                  \n[23] Biostrings_2.70.1                  XVector_0.42.0                    \n[25] SummarizedExperiment_1.32.0        Biobase_2.62.0                    \n[27] MatrixGenerics_1.14.0              matrixStats_1.4.1                 \n[29] rtracklayer_1.62.0                 GenomicRanges_1.54.1              \n[31] GenomeInfoDb_1.38.1                IRanges_2.36.0                    \n[33] S4Vectors_0.40.2                   BiocGenerics_0.48.1               \n[35] reticulate_1.40.0                 \n\nloaded via a namespace (and not attached):\n [1] DBI_1.2.3                   bitops_1.0-9               \n [3] rlang_1.1.4                 magrittr_2.0.3             \n [5] compiler_4.3.3              RSQLite_2.3.9              \n [7] systemfonts_1.1.0           png_0.1-8                  \n [9] vctrs_0.6.5                 reshape2_1.4.4             \n[11] pkgconfig_2.0.3             crayon_1.5.3               \n[13] fastmap_1.2.0               labeling_0.4.3             \n[15] caTools_1.18.3              utf8_1.2.4                 \n[17] rmarkdown_2.29              pracma_2.4.4               \n[19] tzdb_0.4.0                  ragg_1.3.3                 \n[21] DirichletMultinomial_1.44.0 bit_4.5.0.1                \n[23] xfun_0.49                   zlibbioc_1.48.0            \n[25] cachem_1.1.0                CNEr_1.38.0                \n[27] jsonlite_1.8.9              blob_1.2.4                 \n[29] DelayedArray_0.28.0         BiocParallel_1.36.0        \n[31] parallel_4.3.3              R6_2.5.1                   \n[33] stringi_1.8.4               parallelly_1.40.1          \n[35] Rcpp_1.0.13-1               knitr_1.49                 \n[37] R.utils_2.12.3              Matrix_1.6-5               \n[39] timechange_0.3.0            tidyselect_1.2.1           \n[41] abind_1.4-5                 yaml_2.3.10                \n[43] codetools_0.2-20            listenv_0.9.1              \n[45] lattice_0.22-6              plyr_1.8.9                 \n[47] KEGGREST_1.42.0             withr_3.0.2                \n[49] evaluate_1.0.1              pillar_1.9.0               \n[51] generics_0.1.3              RCurl_1.98-1.16            \n[53] hms_1.1.3                   munsell_0.5.1              \n[55] scales_1.3.0                xtable_1.8-4               \n[57] globals_0.16.3              gtools_3.9.5               \n[59] glue_1.8.0                  seqLogo_1.68.0             \n[61] tools_4.3.3                 TFMPvalue_0.0.9            \n[63] annotate_1.80.0             locfit_1.5-9.10            \n[65] XML_3.99-0.17               poweRlaw_0.80.0            \n[67] grid_4.3.3                  AnnotationDbi_1.64.1       \n[69] colorspace_2.1-1            GenomeInfoDbData_1.2.11    \n[71] restfulr_0.0.15             ggdark_0.2.1               \n[73] cli_3.6.3                   rappdirs_0.3.3             \n[75] textshaping_0.4.0           fansi_1.0.6                \n[77] S4Arrays_1.2.0              gtable_0.3.6               \n[79] R.methodsS3_1.8.2           DESeq2_1.42.0              \n[81] digest_0.6.37               SparseArray_1.2.2          \n[83] farver_2.1.2                rjson_0.2.23               \n[85] htmlwidgets_1.6.4           R.oo_1.27.0                \n[87] memoise_2.0.1               htmltools_0.5.8.1          \n[89] lifecycle_1.0.4             httr_1.4.7                 \n[91] GO.db_3.18.0                bit64_4.5.2"
  },
  {
    "objectID": "code/04_training.html",
    "href": "code/04_training.html",
    "title": "Training and Metrics",
    "section": "",
    "text": "Here we train two neural networks\n\nPredict only the shape of the TF ChIP-Nexus profiles\n\n\nPredict both shape and total count of the TF ChIP-Nexus profiles"
  },
  {
    "objectID": "code/04_training.html#train-loop",
    "href": "code/04_training.html#train-loop",
    "title": "Training and Metrics",
    "section": "5.1 Train Loop",
    "text": "5.1 Train Loop\n\n\nCode\nmodel = BPNet(n_dil_layers=9, TF_list=conf_dict[\"tf_list\"], pred_total=False, bias_track=True).to(device)\noptimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\ntrain_loader=DataLoader(train_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=True, \n                        num_workers=0, pin_memory=True)\ntune_loader=DataLoader(tune_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=False, \n                       num_workers=0, pin_memory=True)\n\ntrain_loss, test_loss = [], []\npatience_counter = 0\n\nfor epoch in range(conf_dict[\"max_epochs\"]):\n  print(epoch)\n\n  # test\n  test_loss_epoch = []\n  with torch.no_grad():\n      for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n          one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n          profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n          loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n          test_loss_epoch.append(loss.item())\n      test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n  # train\n  model.train()\n  train_loss_epoch = []\n  for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n    one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n    optimizer.zero_grad()\n    profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n    loss = neg_log_multinomial(k_obs=tf_counts, p_pred=profile_pred, device=device)\n    train_loss_epoch.append(loss.item())\n    loss.backward()\n    optimizer.step()\n  train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n  if test_loss[-1] &gt; np.array(test_loss).min():\n    patience_counter += 1\n  else:\n    patience_counter = 0\n    best_state_dict = model.state_dict()\n\n  if patience_counter == conf_dict[\"early_stop_patience\"]:\n    break\n\nif conf_dict[\"restore_best_weights\"]:\n  model.load_state_dict(best_state_dict)\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24"
  },
  {
    "objectID": "code/04_training.html#train-and-tune-loss",
    "href": "code/04_training.html#train-and-tune-loss",
    "title": "Training and Metrics",
    "section": "5.2 Train and Tune Loss",
    "text": "5.2 Train and Tune Loss\n\n\nCode\ndf = pd.DataFrame({\"epoch\": np.arange(1, epoch+2), \"train\": train_loss, \"test\": test_loss})\ndf.to_csv(STATS_DIR / \"shape_loss.csv\")\nplt.plot(np.arange(epoch+1), np.array(train_loss), label=\"train\")\nplt.plot(np.arange(epoch+1), np.array(test_loss), label=\"test\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "code/04_training.html#save-model",
    "href": "code/04_training.html#save-model",
    "title": "Training and Metrics",
    "section": "5.3 Save Model",
    "text": "5.3 Save Model\n\n\nCode\ntorch.save(obj=model, f=MODELS_DIR / \"all_tfs_shape_model.pt\")"
  },
  {
    "objectID": "code/04_training.html#evaluation",
    "href": "code/04_training.html#evaluation",
    "title": "Training and Metrics",
    "section": "5.4 Evaluation",
    "text": "5.4 Evaluation\n\n\nCode\nmodel = torch.load(MODELS_DIR / \"all_tfs_shape_model.pt\")\n\n\n/tmp/ipykernel_1921712/2386185304.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / \"all_tfs_shape_model.pt\")\n\n\n\n5.4.1 Check Examples\nPlotting the real counts and the predictions for the first batch from the tune dataset.\n\n\nCode\ntune_loader=DataLoader(tune_dataset, batch_size=10, shuffle=False, num_workers=0, pin_memory=True)\none_hot, tf_counts, ctrl_counts, ctrl_smooth = next(tune_loader.__iter__())\none_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\nprofile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth).to(\"cpu\").detach().numpy()\ntf_counts = tf_counts.to(\"cpu\").detach().numpy()\nscaled_pred = profile_pred * tf_counts.sum(axis=-1)[:,:,:,None]\nlw = 0.8\n\n\n\n5.4.1.1 Nanog\n\n\nCode\ntf = 0\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.2 Klf4\n\n\nCode\ntf = 1\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.3 Oct4\n\n\nCode\ntf = 2\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.4 Sox2\n\n\nCode\ntf = 3\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Precision and Recall\n\n\nCode\ntest_pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\nwith torch.no_grad():\n    for batch_idx, data in enumerate(test_dataloader):\n        #print(batch_idx, batch_idx + 100)#, data)\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        profile_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        #print(profile_pred.shape)\n        start = batch_idx*conf_dict[\"batch_size\"]\n        end = (batch_idx+1)*conf_dict[\"batch_size\"] if (batch_idx+1)*conf_dict[\"batch_size\"] &lt; test_dataset.tf_counts.shape[0] else test_dataset.tf_counts.shape[0]\n        test_pred[start:end, :, :, :] = profile_pred\n\n\n\n\nCode\ndef plot_prc(test_dataset, test_pred, tf_index, tf_name, plot = True):\n    true_counts = test_dataset.tf_counts.copy()\n    #subset for one tf\n    tf_counts = true_counts[:, tf_index, :, :]\n    test_pred = test_pred.cpu().numpy().copy()\n    assert np.allclose(test_pred.sum(axis=-1), 1)\n    # subset for one tf\n    tf_pred = test_pred[:, tf_index, :, :]\n    binary_labels, pred_subset, random = binary_labels_from_counts(tf_counts, tf_pred, verbose=False)\n    precision, recall, thresholds = skm.precision_recall_curve(binary_labels, pred_subset)\n    if plot:\n        plt.plot(precision, recall,  label=f\"{tf}\")\n        plt.title(f\"Precision-Recall Curve\")\n        plt.xlabel(\"recall\")\n        plt.ylabel(\"precision\")\n        plt.legend()\n    else:\n        return precision, recall, thresholds\n\n\n\n\nCode\nfor i, tf in enumerate(conf_dict[\"tf_list\"]):\n    plot_prc(test_dataset, test_pred, i, tf, plot=True)\nplt.show()\n\n\n/home/pschaefer/miniconda3/envs/py-env-bpnet/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_list = []\nfor i, tf in enumerate(conf_dict[\"tf_list\"]):\n    precision, recall, thresholds = plot_prc(test_dataset, test_pred, i, tf, plot=False)\n    tmp_df = pd.DataFrame({\n      \"TF\": tf,\n      \"precision\": precision,\n      \"recall\": recall,\n    })\n    df_list.append(tmp_df)\ndf = pd.concat(df_list)\ndf.to_csv(STATS_DIR / \"pr_curve_all_tfs_shape_model.csv\", index=False)\ndel df, tmp_df\n\n\n\n\nCode\n# loop over all four TFs:\ntrue_counts = test_dataset.tf_counts.copy()\nall_pred = test_pred.cpu().numpy().copy()\npatchcap = test_dataset.ctrl_counts.copy()\nassert np.allclose(all_pred.sum(axis=-1), 1)\n\nfor tf_index, tf in enumerate(conf_dict[\"tf_list\"]):\n    patchcap_cp = patchcap.copy()\n    # subset for one tf\n    pred = all_pred[:, tf_index, :, :]\n    counts = true_counts[:, tf_index, :, :]\n    # compute auPRC fro all bins\n    all = compute_auprc_bins(counts, pred, patchcap_cp, verbose=False)\n    df = pd.DataFrame(all)\n    df.to_csv(STATS_DIR / f\"binsizes_auprc_{tf}_shape_model.csv\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"auprc\"], label=\"BPNet\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"random_auprc\"], label=\"random profile\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"average_auprc\"], label=\"average profile\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"patchcap_auprc\"], label=\"PATCH-CAP\")\n    plt.title(f\"{tf}\")\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "code/04_training.html#train-loop-1",
    "href": "code/04_training.html#train-loop-1",
    "title": "Training and Metrics",
    "section": "6.1 Train Loop",
    "text": "6.1 Train Loop\n\n\nCode\nmodel = BPNet(n_dil_layers=9, TF_list=conf_dict[\"tf_list\"], pred_total=True, bias_track=True).to(device)\noptimizer = optim.Adam(model.parameters(), lr=4*1e-4)\n\ntrain_loader=DataLoader(train_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\ntune_loader=DataLoader(tune_dataset, batch_size=conf_dict[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n\ntrain_shape_loss, train_count_loss, train_loss = [], [], []\ntest_shape_loss, test_count_loss, test_loss = [], [], []\n\npatience_counter = 0\n\nfor epoch in range(conf_dict[\"max_epochs\"]):\n\n  # test\n  test_shape_loss_epoch, test_count_loss_epoch, test_loss_epoch = [], [], []\n  with torch.no_grad():\n    for one_hot, tf_counts, ctrl_counts, ctrl_smooth in tune_loader:\n      one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n      shape_pred, count_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n      shape_loss = neg_log_multinomial(k_obs=tf_counts, p_pred=shape_pred, device=device)\n      if count_pred.min() &lt; 0:\n        break\n      count_loss = ((torch.log(1 + count_pred) - torch.log(1 + tf_counts.sum(axis=-1)))**2).mean()\n      loss = shape_loss + lambda_param * count_loss\n      test_shape_loss_epoch.append(shape_loss.item())\n      test_count_loss_epoch.append(count_loss.item())\n      test_loss_epoch.append(loss.item())\n    test_shape_loss.append(sum(test_shape_loss_epoch)/len(test_shape_loss_epoch))\n    test_count_loss.append(sum(test_count_loss_epoch)/len(test_count_loss_epoch))\n    test_loss.append(sum(test_loss_epoch)/len(test_loss_epoch))\n\n  # train\n  model.train()\n  train_shape_loss_epoch, train_count_loss_epoch, train_loss_epoch = [], [], []\n  for one_hot, tf_counts, ctrl_counts, ctrl_smooth in train_loader:\n    one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n    optimizer.zero_grad()\n    shape_pred, count_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n    shape_loss = neg_log_multinomial(k_obs=tf_counts, p_pred=shape_pred, device=device)\n    count_loss = ((torch.log(1 + count_pred) - torch.log(1 + tf_counts.sum(axis=-1)))**2).mean()\n    loss = shape_loss + lambda_param * count_loss\n    train_shape_loss_epoch.append(shape_loss.item())\n    train_count_loss_epoch.append(count_loss.item())\n    train_loss_epoch.append(loss.item())\n    loss.backward()\n    optimizer.step()\n  train_shape_loss.append(sum(train_shape_loss_epoch)/len(train_shape_loss_epoch))\n  train_count_loss.append(sum(train_count_loss_epoch)/len(train_count_loss_epoch))\n  train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n\n  if test_loss[-1] &gt; np.array(test_loss).min():\n    patience_counter += 1\n  else:\n    patience_counter = 0\n    best_state_dict = model.state_dict()\n\n  if patience_counter == conf_dict[\"early_stop_patience\"]:\n    break\n\nif conf_dict[\"restore_best_weights\"]:\n  model.load_state_dict(best_state_dict)"
  },
  {
    "objectID": "code/04_training.html#train-and-tune-loss-1",
    "href": "code/04_training.html#train-and-tune-loss-1",
    "title": "Training and Metrics",
    "section": "6.2 Train and Tune Loss",
    "text": "6.2 Train and Tune Loss\n\n\nCode\ndf = pd.DataFrame({\"epoch\": np.arange(1, epoch+2), \"train_shape\": train_shape_loss, \"test_shape\": test_shape_loss, \"train_count\": train_count_loss, \"test_count\": test_count_loss, \"train\": train_loss, \"test\": test_loss})\ndf.to_csv(STATS_DIR / \"shape_counts_loss.csv\")\nfig, axis = plt.subplots(1, 3, figsize=(12, 3))\naxis[0].plot(np.arange(1, epoch+2), np.array(train_shape_loss), label=\"train\")\naxis[0].plot(np.arange(1, epoch+2), np.array(test_shape_loss), label=\"test\")\naxis[0].set_xlabel(\"Epoch\")\naxis[0].set_ylabel(\"Loss\")\naxis[0].set_title(\"Shape Loss\")\n\naxis[1].plot(np.arange(1, epoch+2), np.array(train_count_loss), label=\"train\")\naxis[1].plot(np.arange(1, epoch+2), np.array(test_count_loss), label=\"test\")\naxis[1].set_xlabel(\"Epoch\")\naxis[1].set_ylabel(\"Loss\")\naxis[1].set_title(\"Count Loss\")\n\naxis[2].plot(np.arange(1, epoch+2), np.array(train_loss), label=\"train\")\naxis[2].plot(np.arange(1, epoch+2), np.array(test_loss), label=\"test\")\naxis[2].set_xlabel(\"Epoch\")\naxis[2].set_ylabel(\"Loss\")\naxis[2].set_title(\"Combined Loss\")\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "code/04_training.html#save-model-1",
    "href": "code/04_training.html#save-model-1",
    "title": "Training and Metrics",
    "section": "6.3 Save Model",
    "text": "6.3 Save Model\n\n\nCode\ntorch.save(obj=model, f=MODELS_DIR / \"all_tfs_model.pt\")"
  },
  {
    "objectID": "code/04_training.html#evaluation-1",
    "href": "code/04_training.html#evaluation-1",
    "title": "Training and Metrics",
    "section": "6.4 Evaluation",
    "text": "6.4 Evaluation\n\n\nCode\nmodel = torch.load(MODELS_DIR / \"all_tfs_model.pt\")\n\n\n/tmp/ipykernel_1921712/1827116589.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / \"all_tfs_model.pt\")\n\n\n\n6.4.1 Check Examples\nPlotting the real counts and the predictions for the first batch from the tune dataset.\n\n\nCode\ntune_loader=DataLoader(tune_dataset, batch_size=10, shuffle=False, num_workers=0, pin_memory=True)\none_hot, tf_counts, ctrl_counts, ctrl_smooth = next(tune_loader.__iter__())\none_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\nprofile_pred, _ = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\nprofile_pred = profile_pred.to(\"cpu\").detach().numpy()\ntf_counts = tf_counts.to(\"cpu\").detach().numpy()\nscaled_pred = profile_pred * tf_counts.sum(axis=-1)[:,:,:,None]\nlw = 0.8\n\n\n\n6.4.1.1 Specific Sequence\n\n\nCode\ntmp_df = test_dataset.region_info.copy().reset_index()\nidx = tmp_df.loc[(tmp_df.seqnames==\"chr1\") & (tmp_df.start &gt; 180924752-1000) & (tmp_df.end &lt; 180925152+1000) & (tmp_df.TF == \"Sox2\")].index.to_numpy()[0]\nprint(tmp_df.iloc[idx])\n\nshape_pred, count_pred = model.forward(torch.from_numpy(test_dataset.one_hot_seqs[idx:idx+1, ]).to(device), torch.from_numpy(test_dataset.ctrl_counts[idx:idx+1, ]).to(device), torch.from_numpy(test_dataset.ctrl_counts_smooth[idx:idx+1, ]).to(device))\n\nshape_pred = shape_pred.cpu().detach().numpy()\n\ndf_list = []\nfor data, kind in zip([test_dataset.tf_counts[idx], shape_pred[0]], [\"counts\", \"prediction\"]):\n  for i, tf in enumerate(conf_dict[\"tf_list\"]):\n    for j, strand in enumerate([\"pos\", \"neg\"]):\n      tmp_df = pd.DataFrame({\"position\": np.arange(1000), \"TF\": tf, \"strand\": strand, \"kind\": kind, \"value\": data[i, j]})\n      df_list.append(tmp_df)\ndf = pd.concat(df_list)\ndf.to_csv(STATS_DIR / \"example_shape_prediction.csv\")\n\n\nindex                                97\nseqnames                           chr1\nstart                         180924435\nend                           180925434\nwidth                              1000\nstrand                                *\nTF                                 Sox2\nset                                test\nname                                NaN\nscore                              1000\nsignalValue                    17.39712\npValue                        441.17929\nqValue                        436.06531\npeak                                404\nRegion         chr1:180924435-180925434\nName: 19, dtype: object\n\n\n\n\n6.4.1.2 Nanog\n\n\nCode\ntf = 0\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.1.3 Klf4\n\n\nCode\ntf = 1\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.1.4 Oct4\n\n\nCode\ntf = 2\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.1.5 Sox2\n\n\nCode\ntf = 3\nfor i in range(profile_pred.shape[0]):\n  fig, axis = plt.subplots(1, 3, figsize=(16, 4))\n  axis[0].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[0].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[1].plot(tf_counts[i, tf, 0], label=\"chip counts\", color=\"green\", linewidth=lw)\n  axis[1].plot(-tf_counts[i, tf, 1], color=\"green\", linewidth=lw)\n  axis[0].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[0].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[2].plot(scaled_pred[i, tf, 0], label=\"scaled pred\", color=\"blue\", linewidth=lw)\n  axis[2].plot(-scaled_pred[i, tf, 1], color=\"blue\", linewidth=lw)\n  axis[0].legend()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.2 Precision and Recall\n\n\nCode\ntest_pred = torch.zeros(test_dataset.tf_counts.shape, dtype=torch.float32).to(device)\ntest_count_pred = torch.zeros(test_dataset.tf_counts.shape[0:3], dtype=torch.float32).to(device)\nwith torch.no_grad():\n    for batch_idx, data in enumerate(test_dataloader):\n        #print(batch_idx, batch_idx + 100)#, data)\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = data\n        one_hot, tf_counts, ctrl_counts, ctrl_smooth = one_hot.to(device), tf_counts.to(device), ctrl_counts.to(device), ctrl_smooth.to(device)\n        profile_pred, count_pred = model.forward(sequence=one_hot, bias_raw=ctrl_counts, bias_smooth=ctrl_smooth)\n        #print(profile_pred.shape)\n        start = batch_idx*conf_dict[\"batch_size\"]\n        end = (batch_idx+1)*conf_dict[\"batch_size\"] if (batch_idx+1)*conf_dict[\"batch_size\"] &lt; test_dataset.tf_counts.shape[0] else test_dataset.tf_counts.shape[0]\n        test_pred[start:end] = profile_pred\n        test_count_pred[start:end] = count_pred\n\n\n\n\nCode\ndef plot_prc(test_dataset, test_pred, tf_index, tf_name, plot = True):\n    true_counts = test_dataset.tf_counts.copy()\n    #subset for one tf\n    tf_counts = true_counts[:, tf_index, :, :]\n    test_pred = test_pred.cpu().numpy().copy()\n    assert np.allclose(test_pred.sum(axis=-1), 1)\n    # subset for one tf\n    tf_pred = test_pred[:, tf_index, :, :]\n    binary_labels, pred_subset, random = binary_labels_from_counts(tf_counts, tf_pred, verbose=False)\n    precision, recall, thresholds = skm.precision_recall_curve(binary_labels, pred_subset)\n    if plot:\n        plt.plot(recall, precision,  label=f\"{tf}\")\n        plt.title(f\"Precision-Recall Curve: {tf_name}\")\n        plt.xlabel(\"recall\")\n        plt.ylabel(\"precision\")\n    else:\n        return precision, recall, thresholds\n\n\n\n\nCode\nfor i, tf in enumerate(conf_dict[\"tf_list\"]):\n    plot_prc(test_dataset, test_pred, i, tf, plot=True)\n    plt.legend()\n\n\n/home/pschaefer/miniconda3/envs/py-env-bpnet/lib/python3.12/site-packages/IPython/core/events.py:82: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  func(*args, **kwargs)\n/home/pschaefer/miniconda3/envs/py-env-bpnet/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_list = []\nfor i, tf in enumerate(conf_dict[\"tf_list\"]):\n    precision, recall, thresholds = plot_prc(test_dataset, test_pred, i, tf, plot=False)\n    tmp_df = pd.DataFrame({\n      \"TF\": tf,\n      \"precision\": precision,\n      \"recall\": recall,\n    })\n    df_list.append(tmp_df)\ndf = pd.concat(df_list)\ndf.to_csv(STATS_DIR / \"pr_curve_all_tfs_count_model.csv\", index=False)\ndel df, tmp_df\n\n\n\n\nCode\n# loop over all four TFs:\ntrue_counts = test_dataset.tf_counts.copy()\nall_pred = test_pred.cpu().numpy().copy()\npatchcap = test_dataset.ctrl_counts.copy()\nassert np.allclose(all_pred.sum(axis=-1), 1)\n\nfor tf_index, tf in enumerate(conf_dict[\"tf_list\"]):\n    patchcap_cp = patchcap.copy()\n    # subset for one tf\n    pred = all_pred[:, tf_index, :, :]\n    counts = true_counts[:, tf_index, :, :]\n    # compute auPRC fro all bins\n    all = compute_auprc_bins(counts, pred, patchcap_cp, verbose=False)\n    df = pd.DataFrame(all)\n    df.to_csv(STATS_DIR / f\"binsizes_auprc_{tf}_count_model.csv\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"auprc\"], label=\"BPNet\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"random_auprc\"], label=\"random profile\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"average_auprc\"], label=\"average profile\")\n    sns.scatterplot(x=df[\"binsize\"], y=df[\"patchcap_auprc\"], label=\"PATCH-CAP\")\n    plt.title(f\"{tf}\")\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.3 MSE and R2\n\n\nCode\ntrue_total_counts = test_dataset.tf_counts.sum(axis=-1).copy()\npred_total_counts = test_count_pred.cpu().detach().numpy()\ntf_means = true_total_counts.mean(axis=0)\n\ndf = pd.DataFrame(columns=[\"TF\", \"mse\", \"tss\", \"rss\", \"r2\"])\nfor i, tf in enumerate(conf_dict[\"tf_list\"]):\n  mse = ((np.log1p(true_total_counts[:, i]) - np.log1p(pred_total_counts[:, i]))**2).mean()\n  tss = ((true_total_counts[:, i] - tf_means[None, i])**2).sum()\n  rss = ((true_total_counts[:, i] - pred_total_counts[:, i])**2).sum()\n  r2 = 1 - rss/tss\n  tmp_df = pd.DataFrame({\"TF\": tf, \"mse\": mse, \"tss\": tss, \"rss\": rss, \"r2\": r2}, index=[0])\n  df = pd.concat([df, tmp_df], ignore_index=True, axis=0)\ndf.to_csv(STATS_DIR / \"count_stats.csv\", index=False)\n\n\n/tmp/ipykernel_1921712/2810987182.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  df = pd.concat([df, tmp_df], ignore_index=True, axis=0)"
  },
  {
    "objectID": "code/06_interpretability.html",
    "href": "code/06_interpretability.html",
    "title": "Interpretability",
    "section": "",
    "text": "1 Description\n\nHere we use Interpretable ML tools to make sense of the model predictions.\n\n\n\n2 Libraries\n\n\nCode\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom captum.attr import DeepLift\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport modisco.visualization\nfrom modisco.visualization import viz_sequence\n\nfrom src.config import conf_dict\nfrom src.architectures import *\nfrom src.utils import * \nfrom src.loss import *\nfrom src.metrics import *\nfrom src.DeepLiftUtils import *\n\n# dark mode doesn't quite work for the attribution plots\n#plt.style.use('dark_background')\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n3 Data\n\n\nCode\n#conf_dict[\"tf_list\"] = [\"Nanog\"]\n#conf_dict[\"batch_size\"] = 248\n#conf_dict[\"max_epochs\"] = 25\n#conf_dict[\"early_stop_patience\"] = 4\n#conf_dict[\"restore_best_weights\"] = True\n\nPRC_DIR = Path(\"..\") / \"prc\"\nMODELS_DIR = Path(\"..\") / \"trained_models\"\nFIG_DIR = Path(\"..\") / \"figures\" / \"interpretability\"\nFIG_DIR.mkdir(exist_ok=True, parents=True)\n\n\n\n\nCode\nif torch.cuda.is_available():\n  device = \"cuda\"\nelif torch.backends.mps.is_available():\n  device = torch.device(\"mps\")\nelse:\n  decive = \"cpu\"\nprint(f\"Using {device} device\")\n\n\nUsing mps device\n\n\n\n\nCode\ntrain_dataset = ChIP_Nexus_Dataset(set_name=\"train\", \n                                   input_dir=PRC_DIR, \n                                   TF_list=conf_dict[\"tf_list\"])\ntrain_dataset\n\n\nChIP_Nexus_Dataset\nSet: train\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 93904\n\n\n\n\nCode\ntune_dataset = ChIP_Nexus_Dataset(set_name=\"tune\", \n                                  input_dir=PRC_DIR, \n                                  TF_list=conf_dict[\"tf_list\"])\ntune_dataset\n\n\nChIP_Nexus_Dataset\nSet: tune\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 29277\n\n\n\n\nCode\ntest_dataset = ChIP_Nexus_Dataset(set_name=\"test\", \n                                  input_dir=PRC_DIR, \n                                  TF_list=conf_dict[\"tf_list\"])\ntest_dataset\n\n\nChIP_Nexus_Dataset\nSet: test\nTFs: ['Nanog', 'Klf4', 'Oct4', 'Sox2']\nSize: 27727\n\n\nFor interpretability methods like DeepLift (https://github.com/kundajelab/deeplift), integrated gradients or gradient x input, we require a scalar output. The BPNet model does, however, predict profiles shapes as tensors of size 2 x 1000 (strand x bps). For backpropagation or DeepLift we collapse the profile for each strand to one representative value, which can then be used to compute the gradient of this scalar with respect to each of the input bps.\nWe define our profile prediction as the softmax of the pre-activation of the last layer in our profile shape prediction head.\n\\(\\tilde{z}\\) is the pre-activation of our last layer.\n\\(p = softmax(\\tilde{z}) = \\frac{\\exp{\\tilde{z}}}{\\sum_{i}^{N}{\\exp{\\tilde{z'}_i}}}\\) with \\(N\\) corresponding to the number of bps.\nWe weight the pre-activations of the last layer with the softmax of the same pre-activations and take the sum. This way we get one value for each strand.\n\\(z = \\sum_{i}^{N}{p_i * \\tilde{z}_i}\\) with \\(z \\in \\mathbb{R}^{2}\\)\nImportantly, we have to detach the softmax activation so that it is a constant value during backpropagation of contribution scores or gradients.\nWhen computing DeepLift scores we take the average of the two strands.\n\n\nCode\nmodel = torch.load(MODELS_DIR / \"all_tfs_model.pt\")\n\n\n/var/folders/n8/xnf8qmpj41795tf723xy8mqm0000gn/T/ipykernel_94928/1827116589.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(MODELS_DIR / \"all_tfs_model.pt\")\n\n\n\n\n4 DeepLift\nWe picked some exemplary regions of interest from the paper (Fig.2 and Supp.Fig.2) to compute the contribution scores and make some plots.\n\n\nCode\ndl = DeepLift(model)\n\n\n\n\nCode\ndef get_contr_region(seqname, start, end, dataset, model, dl, device, tf_list, output_dir, plot=True, figsize1=(20,2), figsize2=(10,1.5)):\n    \"\"\"Compute the DeepLift contribution scores for a 1kb sequence which contains the shorter \n    region of interest specified by the input arguments.\n    Params:\n        seq_name: string\n            specifies the chromosome \n        start: int\n            specifies start coordinate of sequence of interest on chromosome\n        end: int\n            specifies end coordinate of sequence of interest on chromsome\n        dataset: utils.ChIP_Nexus_Dataset object\n        device: cuda or cpu\n        tf_list: \n            Contains names of TFs for which we want to compute the contributions\n        plot: bool\n            Whether to visualize the DeepLift contribution scores.\n\n    Returns:\n        contr: tensor (4x1000)\n            Contains the contribution of each bp to the profile shape predictions for the input sequence.\n        dist_start: int\n            distance between the start of the 1kb sequence and the region of interest\n    \"\"\"\n    # select sequence of interest\n    tmp_df, idx, dist_start, one_hot, baseline, bias_raw, bias_smooth, tf_counts = get_seq_oi(seqname, start, end, dataset, device)\n    width = end - start\n\n    # compute contribution scores for each tf\n    contr_list = []\n    df_list = []\n    for tf_index, tf in enumerate(tf_list):\n        contr = dl.attribute(inputs=one_hot, baselines=baseline, target=(tf_index), additional_forward_args=(bias_raw, bias_smooth, True)).detach().cpu().numpy()\n        contr_list.append(contr)\n\n        pred, _ = model.forward(one_hot, bias_raw, bias_smooth, interpretation=False)\n        pred = pred.detach().cpu().numpy().squeeze()\n        # scale prediciton with total counts\n        pred = pred * tf_counts.sum(axis=-1, keepdims=True)\n        tf_df = pd.DataFrame({\"pos\": np.arange(width+1), \"TF\": tf, \"pos_values\": pred[tf_index, 0, dist_start : (dist_start + width+1)], \"neg_values\": pred[tf_index, 1, dist_start : (dist_start + width+1)]})\n        df_list.append(tf_df)\n\n        if plot:\n            # entire sequence original\n            plot_weights(contr,\n            fontsizes=[20,15,15],\n            title = f\"{tf} - 1kbp sequence\", \n            xlabel=f\"{tmp_df.seqnames[idx]}: {tmp_df.start[idx]}-{tmp_df.end[idx]}\", \n            ylabel=\"DeepLift contribution scores\",\n            subticks_frequency=20, figsize=figsize1)\n            plt.savefig(output_dir / f\"{tf}_{seqname}_{start}_{end}_entireSeq_DeepLift.pdf\")\n\n            # zoomed into motif region\n            plot_weights(contr[:, :,dist_start : (dist_start + width+1)],\n            fontsizes=[20,15,15],\n            title = f\"{tf} - Motif of interest\", \n            xlabel=f\"{seqname}: {start}-{end}, ({dist_start} - {dist_start + width+1})\", \n            ylabel=\"DeepLift contribution scores\",\n            subticks_frequency=10, figsize=figsize2)\n            plt.savefig(output_dir / f\"{tf}_{seqname}_{start}_{end}_zoomedSeq_DeepLift.pdf\")  \n            \n            # plot profiles\n            fig, axis = plt.subplots(1,2,figsize=(12,4))\n            axis[0].plot(tf_counts[tf_index, 0, :], label=\"true counts\", color=\"green\", linewidth=0.8)\n            axis[0].plot(-tf_counts[tf_index, 1, :], color=\"green\", linewidth=0.8)\n            axis[0].plot(pred[tf_index, 0, :], label=\"pred\", color=\"blue\", linewidth=0.8)\n            axis[0].plot(-pred[tf_index, 1, :], color=\"blue\", linewidth=0.8)   \n            axis[0].set_xlabel(\"bp\")\n            axis[0].set_ylabel(\"Read counts\")\n            axis[1].plot(pred[tf_index, 0, :], label=\"pred\", color=\"blue\", linewidth=0.8)\n            axis[1].plot(-pred[tf_index, 1, :], color=\"blue\", linewidth=0.8)\n            axis[1].set_xlabel(\"bp\")\n            axis[1].set_ylabel(\"Predicted probabilitiy * total counts\")\n            axis[0].legend()\n            axis[1].legend()\n            plt.show()\n    plot_df = pd.concat(df_list)\n            \n    return contr, dist_start, plot_df\n\n\n\n4.0.1 Klf4 E2 enhancer (Supplementary Fig.2)\n\n\nCode\ncontr, dist_start, pred = get_contr_region(\"chr4\", start=55475545, end=55475604, dataset=tune_dataset, output_dir=FIG_DIR, dl=dl, tf_list=conf_dict[\"tf_list\"], device=device,   model=model)\n#pred.to_csv(\"/home/kathi/AML_Project/data/test_fig.csv\")\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.0.2 Nanog enhancer (Supplementary Fig.2)\n\n\nCode\ncontr, dist_start, pred = get_contr_region(\"chr6\", start=122707394, end=122707454, dataset=train_dataset, output_dir=FIG_DIR, dl=dl, tf_list=conf_dict[\"tf_list\"], device=device, model=model)\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.0.3 Fbx15 enhancer (Supplementary Fig.2)\n\n\nCode\ncontr, dist_start, pred = get_contr_region(\"chr18\", start=84934461, end=84934521, dataset=train_dataset, output_dir=FIG_DIR, dl=dl, tf_list=conf_dict[\"tf_list\"], device=device, model=model)\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n  warnings.warn(\n/Users/pschafer/miniforge3/envs/py-env-bpnet/lib/python3.12/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n               activations. The hooks and attributes will be removed\n            after the attribution is finished\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Input x Gradient\nFor the same exemplary regions from the paper.\n#{python} grad, grad_in = input_gradient(\"chr4\", start=55475545, end=55475604, dataset=tune_dataset,output_dir=FIG_DIR,  model=model, tf_list=conf_dict[\"tf_list\"], device=device)\n\n5.0.1 Klf4 E2 enhancer (Supplementary Fig.2)\n#{python} grad, grad_in = input_gradient(\"chr4\", start=55475545, end=55475604, dataset=tune_dataset,output_dir=FIG_DIR,   model=model, tf_list=conf_dict[\"tf_list\"], device=device)\n\n\n5.0.2 Nanog enhancer (Supplementary Fig.2)\n#{python} grad, grad_in = input_gradient(\"chr6\", start=122707394, end=122707454, dataset=train_dataset, output_dir=FIG_DIR,  model=model, tf_list=conf_dict[\"tf_list\"], device=device)\n\n\n5.0.3 Fbx15 enhancer (Supplementary Fig.2)\n#{python} grad, grad_in = input_gradient(\"chr17\", start=35504453, end=35504603, dataset=train_dataset, output_dir=FIG_DIR,  model=model, tf_list=conf_dict[\"tf_list\"], device=device)"
  }
]